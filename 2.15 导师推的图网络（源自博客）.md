# 2.15 谢老师推的图网络(源自博客)

https://mp.weixin.qq.com/s/mDSdQhHGH1xF9kFLLluV6w

**1 介绍**

Transformer架构在自然语言处理和计算机视觉等领域表现出色，但在图级预测中表现不佳。为了解决这个问题，**本文介绍了Graphormer，一种基于标准Transformer架构的图表示学习方法，在广泛的图表示学习任务中取得了优异成绩**，特别是在OGB大规模挑战中。

Graphormer的关键见解是将图的结构信息有效地编码到模型中，为此提出了一些简单而有效的结构编码方法。此外，本文还从数学上描述了Graphormer的表现力，并表明许多流行的GNN变体可以作为Graphormer的特例。

**2 预备知识**

本节主要回顾图神经网络和 Transformer 的预备知识：

**图神经网络（GNN）。**GNN通过迭代更新节点的表示来学习节点和图的表示，其中节点的表示通过聚合其第一或更高阶邻居的表示来更新。第l次迭代聚合的特征可以由AGGREGATE-COMBINE步骤表示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTvOXHzobeib01S8icfPia3YFliaic0oNtqeNE71zkmic3Qia1NIDjYLLiaNh7ibGYdatk5s1lTCgcXMUUyTZSA/640?wx_fmt=png&from=appmsg&wxfrom=13&tp=wxpic)



其中N(vi)表示vi的一阶或更高阶邻居的集合，AGGREGATE函数收集邻居信息，常见的聚合函数包括MEAN、MAX、SUM，用于GNN的不同架构，COMBINE函数将邻居信息融合到节点表示中。

此外，设计了一个READOUT函数，将最终迭代的节点特征聚合到整个图的表示中，用于图表示任务。

![图片](https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTvOXHzobeib01S8icfPia3YFliaON1spgKNtD6XnzqUiajJy9GBAxfFRsAM4HT5Eh317KMSgO5k7bQAZAQ/640?wx_fmt=png&from=appmsg&wxfrom=13&tp=wxpic)

READOUT可以通过简单的置换不变函数或更复杂的图级池化函数实现。

**Transformer。**Transformer架构由Transformer层组成，每个层包括自注意力模块和位置前馈网络。自注意力模块将输入H投影到Q、K、V，然后计算自注意力：

![img](https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTvOXHzobeib01S8icfPia3YFlialv4yxEV8nkiaQRZhyoSIxlJptxrQZfiaYibdYggcribdxn99a92v7tGIUg/640?wx_fmt=png&from=appmsg&wxfrom=13&tp=wxpic)



其中A矩阵捕捉查询和键的相似性，简化为单头自注意力，假设dK=dV=d，省略偏置项。



**3 图形生成器**

本节介绍了用于图形任务的Graphormer，详细介绍了Graphormer中的关键设计，提供了Graphormer的详细实现，并证明其优于流行的GNN模型。

**3.1 Graphormer中的结构编码**

Graphormer通过三种简单但有效的编码设计，将图的结构信息引入Transformer模型，提高模型性能。参见图1。

![图片](https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTvOXHzobeib01S8icfPia3YFliaqf0cVYjwCHUI1KCpAkM7y0Hak9SJNxfiar4BTAclBJTwks2OibXjBBDA/640?wx_fmt=png&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)图1 图中的信息是我们的图形式（Graphormer）所提出的关键度编码、空间编码以及边缘编码的描述。

**3.1.1 中心编码**

在方程4中，注意力分布是基于节点之间的语义相关性计算的。然而，节点中心性（衡量节点在图中的重要程度）通常是图理解的重要信号。例如，拥有大量追随者的名人是在预测社交网络趋势的重要因素[40,39]。这些信息在当前注意力计算中被忽略了，我们认为它应该是 transformer 模型的有价值的信号。

在Graphormer中，我们使用度中心性作为神经网络的附加信号，度中心性是文献中的标准中心性度量之一。具体来说，我们开发了一种中心性编码，根据每个节点的入度和出度为其分配两个实值嵌入向量。由于中心性编码应用于每个节点，我们只需将其作为输入添加到节点特征中。

![图片](https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTvOXHzobeib01S8icfPia3YFliaibyMObJEuwWd3kJmyyqxSLEj9tj71ibGrK1DViacomdQdokY6McjyBhAQ/640?wx_fmt=png&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

其中z-、z+∈Rd是分别由入度deg-(vi)和出度deg+(vi)指定的可学习嵌入向量。对于无向图，deg-(vi)和deg+(vi)可以统一为deg(vi)。通过在输入中使用中心性编码，softmax注意力可以捕捉查询和关键中的节点重要性信号。因此，该模型可以在注意力机制中捕捉语义相关性和节点重要性。

**3.1.2 空间编码**

Transformer的全局感受野使其在每个层中每个标记都能关注任何位置的信息。但需要明确指定位置依赖性，如位置编码或相对位置编码。

本文提出了一种新的空间编码方法，用于在模型中编码图的结构信息。具体来说，对于图 G，考虑一个函数 φ (vi , vj ) : V × V → R，它衡量图 G 中 vi 和 vj 之间的空间关系。在本论文中，选择 φ(vi , vj ) 作为 vi 和 vj 之间的最短路径距离（SPD），如果这两个节点是连接的。如果不是，将 φ 的输出设置为 -1。为每个可行输出值分配一个可学习的标量，该标量将作为自注意力模块中的偏置项。将 Aij 表示为查询-关键字乘积矩阵 A 的 (i, j) 元素，我们有

![图片](https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTvOXHzobeib01S8icfPia3YFliauqxr8zibrzEhbcmyym64zUdC2OFaR9day7vyL3qgAOd5DIqEicKYPHJg/640?wx_fmt=png&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

其中bφ(vi,vj)是一个由φ(vi,vj)索引的可学习标量，在所有层之间共享。

我们提出的方法具有以下优势：与第2节中描述的传统GNN相比，Transformer层提供了全局信息，每个节点都可以关注图中的所有其他节点，如方程（6）。此外，每个节点可以根据图结构信息自适应地关注所有其他节点。例如，模型可能会更多地关注它附近的节点，而较少关注远离它的节点。

**3.1.3 注意力中的边缘编码**

在许多图任务中，边具有结构特征，如分子图中的原子对特征。以前的工作主要采用两种边编码方法：将边特征添加到相关节点特征中，或与节点特征一起在聚合中使用。然而，这些方法只将边信息传播到相关节点，可能不是有效利用边信息表示整个图的方法。

本文提出了一种新的边缘编码方法，以更好地将边缘特征编码到注意力层中。该方法考虑了连接节点的边缘，并计算了边缘特征和沿路径的可学习嵌入的点积的平均值。通过偏置项将边缘特征整合到注意力模块中，提高了注意力机制的性能。具体如方程（6），我们修改了方程（3）中A的（i，j）元素，将边缘编码cij修改为：

![图片](https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTvOXHzobeib01S8icfPia3YFliaD1p9FPYrL2rQgZT88iboO0l2Hic4k0kaEHhgqtUHOWSYXP6mP7CB2mwQ/640?wx_fmt=png&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

其中 xen 是 SPij 中第 n 个边 en 的特征，wn E ∈ R dE 是第 n 个权重嵌入，dE 是边特征的维数。

**3.2 Graphomer的实现细节**

**Graphormer层。**Graphormer层建立在Transformer编码器的原始实现上，并在MHA和FFN之前应用了层归一化。对于FFN子层，将输入、输出和内层的维数设置为相同的d维。Graphormer层正式表征如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTvOXHzobeib01S8icfPia3YFliaBNNZoia7oTT2xv3flN5OoVVPMia8UXP7YLgIoZpB71rM05QZSJ3ug8Xw/640?wx_fmt=png&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

**图池化。**在Graphormer中，引入了一个名为[VNode]的特殊节点，与每个节点连接，用于表示图中的普通节点。在AGGREGATE-COMBINE步骤中，[VNode]的表示已更新为图中的普通节点，而整个图的表示hG将是最终层中[VNode]的节点特征。这与BERT模型中的[CLS]标记类似，用于表示下游任务上的序列级特征。虽然[VNode]与图中所有其他节点相连，但连接不是物理的，空间编码重置为可学习的不同标量以区分物理连接和虚拟连接。

**3.3  Graphomer有多强大？**

本章介绍了Graphormer的三种结构编码和架构，并探讨了Graphormer是否比其他GNN变体更强大。通过展示Graphormer可以表示流行GNN模型中的AGGREGATE和COMBINE步骤，给出了肯定的答案。

Graphormer层通过选择适当的权重和距离函数φ，可以表示流行的GNN模型（如GIN、GCN、GraphSAGE）的AGGREGATE和COMBINE步骤。这一结果通过空间编码使自注意力模块能够区分节点vi的邻居集N（vi），计算N（vi）的均值统计，并将多个头和FFN应用于vi和N（vi）的表示来实现。Graphormer可以超越经典的消息传递GNN，其表达能力不超过1-Weisfeiler-Lehman（WL）测试。

**自注意和虚拟节点之间存在联系。**虚拟节点技巧通过添加超节点增强图，提高GNNs性能。自注意可实现图级聚合和传播，无需额外编码。

Graphormer层通过选择适当权重，每个节点表示可表示平均读出函数，无需额外编码。利用自注意力，可模拟图级读出操作，聚合整个图信息。实验发现Graphormer未出现过度平滑问题，具有可扩展性。启发引入特殊节点用于图读出。

**4  实验**

我们在OGB-LSC量子化学回归挑战赛上进行了实验，该挑战赛包含超过380万个图。我们还报告了其他三个流行任务的结果：ogbgmolhiv、ogbg-molpcba和ZINC。数据集和训练策略的详细描述在附录B中。

**4.1 OGB 大规模挑战**

**基线。**Graphormer与GCN、GIN及其变体进行了基准测试，实现了最先进的有效和测试平均绝对误差。此外，Graphormer还与GIN的多跳变体、12层深度图网络DeeperGCN进行了比较，并在其他排行榜上表现出色。最后，Graphormer与基于Transformer的图模型GT进行了比较。

**设置。**我们报告了Graphormer（L = 12，d = 768）和GraphormerSMALL（L = 6，d = 512）两种模型大小的结果。注意力头数和边缘特征dE的维数均为32。使用AdamW优化器，超参数设置为1e-8，（β1，β2）为（0.99，0.999）。峰值学习率为2e-4（GraphormerSMALL为3e-4），预热阶段为60k步，采用线性衰减学习率调度器。总训练步骤为1M，批大小为1024。所有模型在8个NVIDIA V100 GPU上训练约2天。

**结果。**表1比较了PCQM4M-LSC数据集上的性能。GIN-VN实现了最先进的验证MAE 0.1395。GT的原始实现使用了64个隐藏维度来减少参数。为了公平比较，我们还报告了将隐藏维度扩大到768的结果，即GT-Wide，总参数数为83.2M。然而，GT和GT-Wide都不如GIN-VN和DeeperGCN-VN。特别是，我们没有观察到GT的参数增长带来的性能提升。

表1 PCQM4M-LSC 的结果。* 表示结果引用自官方排行榜[21]。

![图片](https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTvOXHzobeib01S8icfPia3YFliaKF9WwsNhwLgZAFvqYVGaI173VYDx2NZX3AYCfDjx2mWp045YXDY6zw/640?wx_fmt=png&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

Graphormer在相对验证MAE上优于GINVN，下降11.5%。通过与ExpC集成，在完整测试集上获得0.1200MAE，并在OGB大规模挑战赛图级赛道获得第一名。Graphormer未出现过度平滑问题，随着模型深度和宽度增加，训练和验证误差持续下降。

**4.2 图表示**

本节研究了Graphormer在流行排行榜的图级预测任务（OGB和ZINC）上的性能。在OGB-LSC上预训练Graphormer模型，探索其可转移能力。对于不鼓励大型预训练模型的ZINC，从头开始训练Graphormer slim（L=12，d=80，总参数=489K）。

**基线。**我们报告了GNN在官方排行榜上的最佳性能，无需额外领域特定特征。我们还报告了GIN-VN在PCQM4M-LSC数据集上的性能，该数据集实现了以前最先进的有效和测试MAE。

**设置。**我们在附录B中报告了详细的训练策略。由于模型规模大而数据集规模小，Graphormer容易过拟合。因此，我们使用FLAG数据增广技术来缓解OGB数据集上的过拟合问题。

**结果。**表2、3和4总结了Graphormer在与其他GNN的性能比较。Graphormer在MolHIV、MolPCBA和ZINC数据集上优于其他GNN，包括基于Transformer的GT和SAN。除了Graphormer，其他预训练的GNN没有达到竞争性能，与之前文献一致。附录C有更多比较内容。

表2 MolPCBA 的结果

![图片](https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTvOXHzobeib01S8icfPia3YFliaiacIoMVyXTFycLwzW22jCUaVprQhY8kXUVpLqpYfLA8bBCtX9FRSia3Q/640?wx_fmt=png&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

表3 MolHIV 的结果

![图片](https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTvOXHzobeib01S8icfPia3YFliatjBvG8jdjw3eHrqxAS6Al8ibtJN0dEexDicFC2wN5R7ylesO3IS0Gr6g/640?wx_fmt=png&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

表4 ZINC 的结果

![图片](https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTvOXHzobeib01S8icfPia3YFlialjlLkgicTwzERMQKxtKeasomtd67DOU8MgAfPchyHPCBbVotXU5c0hg/640?wx_fmt=png&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)



**4.3 消融研****究**

在PCQM4M-LSC数据集上，我们使用12层Transformer模型进行100K次迭代训练，并对Graphormer进行了消融研究，结果如表5所示。

表5 不同设计的 PCQM4M-LSC 数据集的消融研究结果

![图片](https://mmbiz.qpic.cn/mmbiz_png/eyibF6kJBjTvOXHzobeib01S8icfPia3YFlian7b5vgOdaInzx4m5XhCXYYs4ElEUzOdhuPvbEeS237BcO1sfkczHicg/640?wx_fmt=png&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

**节点关系编码。**我们比较了位置编码（PE）和空间编码，发现空间编码在Transformer中更有效地编码了不同节点关系的信息。之前的GNN使用了WL-PE和Laplacian PE，我们报告了Laplacian PE的性能，因为它在文献[13]中表现良好。使用空间编码的Transformer架构优于基于位置编码的对应架构，表明空间编码在捕获节点空间信息方面是有效的。

**中心性编码。**基于度数的中心性编码Transformer架构可显著提升性能，表明其对建模图数据至关重要。

**边缘编码。**我们提出的边缘编码（注意力偏差）与两种常用边缘编码（节点和聚合）进行了比较，以将边缘特征整合到GNN中。结果显示，我们提出的方法性能明显优于传统方法，表明作为注意力偏差的边缘编码更有效地捕捉了Transformer的空间信息。

**5 相关工作**

本节重点介绍基于Transformer架构的GNN或图结构编码，较少关注通过注意力机制应用于GNN的工作。

**5.1 图Transformer**

有几篇论文研究了纯Transformer架构在图表示任务上的性能，如[46]对Transformer层进行了修改，使用额外的GNN生成Q、K和V向量，长程残差连接和两个分支的FFN分别产生节点和边缘表示，并在下游任务上微调获得极好的结果。[41]通过将邻接矩阵和原子间距离矩阵添加到注意力概率中，修改了注意力模块。[13]建议在图数据上的Transformer中的注意力机制只应聚合来自邻居的信息，并建议使用拉普拉斯特征向量作为位置编码。[28]提出了一种新颖的全拉普拉斯谱来学习图中每个节点的位置，并从经验上显示了比GT更好的结果。

**5.2  GNN 中的结构编码**

**GNN 中的路径和距离。**GNN中广泛使用路径和距离信息。例如，基于注意力的聚合方法将节点、边、距离和环标志特征结合起来计算注意力概率。另一方法利用基于路径的注意力模拟中心节点与其高阶邻居的影响。还有基于图上距离加权的聚合方案。而距离编码被证明比1-WL测试具有更严格的表达能力。

**图 Transformer 中的位置编码（PE）。**有几项工作引入了位置编码，以帮助基于Transformer的GNNs捕获节点位置信息。Graph-BERT使用了三种类型的PE，包括绝对WL-PE和亲密度和跳跃度两种基于子图的变体。绝对拉普拉斯位置编码在[13]中被采用，且性能超过了[61]中使用的绝对WL-PE。

**边缘特征。**本文还提出了几种利用边缘特征的方法，包括基于注意力的GNN层、将边缘特征编码到GIN、将边缘特征投影到嵌入向量并乘以注意力系数，以及将结果发送到额外的FFN子层以产生边缘表示。

**6 结论**

我们已经探索了将Transformer直接应用于图表示，提出了Graphormer，它在各种流行的基准数据集上表现良好。但仍有挑战，例如自注意力模块的二次复杂性限制了在大图上的应用。未来需要开发高效的Graphormer，并利用基于领域知识的编码来提高性能。此外，需要适用的图采样策略用于Graphormer的节点表示提取，留待未来工作。