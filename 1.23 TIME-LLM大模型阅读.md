# 1.23 TIME-LLM大模型论文阅读

### TIME-LLM: TIME SERIES FORECASTING BY REPROGRAMMING LARGE LANGUAGE MODELS



* Q：什么领域
  * 时间序列预测在许多现实世界的动态系统中具有重要意义，并得到了广泛的研究。与自然语言处理( NLP )和计算机视觉( CV )不同，单个大型模型可以处理多个任务，用于时间序列预测的模型通常是专门的，这就需要为不同的任务和应用设计不同的模型。
* Q：解决了什么挑战（简单）
  * 挑战仍然在于有效地对时间序列数据和自然语言的模态进行对齐，以利用这些能力。
* Q：做了啥
  * 一个重编程框架，将LLM重用于一般的时间序列预测，而主干语言模型保持完整。
    * 我们首先将输入的时间序列与文本原型重新编程，然后将其输入到冻结的LLM中，使两种模态对齐。
  * 为了增强LLM对时间序列数据的推理能力，我们提出了前缀提示( Prompt-as-Prefix，PaP )，它丰富了输入上下文，指导了重新编程的输入补丁的转换。
  * 最后，将LLM变换后的时间序列块进行投影，得到预测结果。
* Q：解决了何种挑战问题
  * 虽然时间序列建模并没有从同样的重大突破中受益，但LLMs令人印象深刻的能力启发了它们在时间序列预测中的应用。
  * 现有的非LLM方法在很大程度上是统计性的，没有太多的先天推理。
  * 因为LLMs操作的是离散的令牌，而时间序列数据本质上是连续的。此外，用于解释时间序列模式的知识和推理能力并不自然地存在于LLMs的预训练中。