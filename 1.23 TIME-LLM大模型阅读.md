# 1.23 TIME-LLM大模型论文阅读

### TIME-LLM: TIME SERIES FORECASTING BY REPROGRAMMING LARGE LANGUAGE MODELS



* Q：什么领域
  * 时间序列预测在许多现实世界的动态系统中具有重要意义，并得到了广泛的研究。与自然语言处理( NLP )和计算机视觉( CV )不同，单个大型模型可以处理多个任务，用于时间序列预测的模型通常是专门的，这就需要为不同的任务和应用设计不同的模型。

* Q：解决了什么挑战（简单）
  * 挑战仍然在于有效地对时间序列数据和自然语言的模态进行对齐，以利用这些能力。

* Q：做了啥
  * 一个重编程框架，将LLM重用于一般的时间序列预测，而主干语言模型保持完整。
    * 我们首先将输入的时间序列与文本原型重新编程，然后将其输入到冻结的LLM中，使两种模态对齐。
  * 为了增强LLM对时间序列数据的推理能力，我们提出了前缀提示( Prompt-as-Prefix，PaP )，它丰富了输入上下文，指导了重新编程的输入补丁的转换。
  * 最后，将LLM变换后的时间序列块进行投影，得到预测结果。

* Q：解决了何种挑战问题
  * 虽然时间序列建模并没有从同样的重大突破中受益，但LLMs令人印象深刻的能力启发了它们在时间序列预测中的应用。
  * 现有的非LLM方法在很大程度上是统计性的，没有太多的先天推理。
  * 因为LLMs操作的是离散的令牌，而时间序列数据本质上是连续的。此外，用于解释时间序列模式的知识和推理能力并不自然地存在于LLMs的预训练中。

* Q：核心贡献
  * 其核心思想是将输入的时间序列重新编程为更自然地适合语言模型能力的文本原型表示。
  * 目标是重新编程一个大的语言模型f ( · )来理解输入的时间序列，并准确地预测H个未来时间步的读数。

* Q：大模型微调是什么
  * 只更新了轻量级输入转换和输出投影的参数，而主干语言模型是冻结的。

* Q：微调的好处是
  * TIME - LLM是直接优化的，只需要少量的时间序列和少量的训练历元就可以获得，与从头开始构建大型特定领域模型或对其进行微调相比，保持了较高的效率并施加了较少的资源约束。

* Q：无需对骨干模型进行任何微调。如何理解

  "Backbone model"在不同的上下文中可能有不同的含义，但在机器学习和深度学习的领域，它通常指的是一个用于特定任务（如图像识别、自然语言处理等）的基础网络结构。这个基础网络（backbone）作为一个核心组件，被用于提取特征，然后可以在其上叠加其他层或模块来完成更复杂的任务。

  在许多深度学习应用中，一个强大的骨干网络是至关重要的，因为它能够从原始数据中有效地提取有用的特征。例如，在图像识别中，常用的骨干网络包括ResNet、VGG、Inception等。这些网络经过大量数据的预训练，能够捕捉到图像中的关键信息，如边缘、纹理和形状。

  "无需对骨干模型进行任何微调"通常意味着在特定应用中直接使用预训练的骨干模型，而不需要对其进行进一步的训练或调整。这在一些情况下是可行的，特别是当预训练模型已经能够很好地捕捉到对当前任务来说重要的特征时。使用未经微调的预训练模型可以节省大量的时间和计算资源，尤其是在资源有限的情况下。然而，这种方法的效果很大程度上取决于预训练模型的质量和与当前任务的相关性。

  骨干模型：backbone model，代指

* 重编程原文

  * 为了弥补这一差距，我们提出利用预训练的词嵌入 $\mathbf{E} \in \mathbb{R}^{V \times D}$ 对骨干模型中的 $\hat{\mathbf{X}}_P^{(i)}$ 进行重编程，其中 $V$ 是词汇量。然而，并没有先验知识指出哪些源词汇是直接相关的。因此，仅仅利用 $\mathbf{E}$ 将导致一个庞大且可能是密集的重编程空间。一个简单的解决方案是通过线性探测 $\mathbf{E}$ 来维护一个小型的文本原型集合，表示为 $\mathbf{E}^{\prime} \in \mathbb{R}^{V^{\prime} \times D}$，其中 $V^{\prime} \ll V$。

  * 如图 3 (a) 所示。文本原型学习连接语言提示，例如“长稳定”（蓝线）和“短上升”（红线），然后结合起来表示局部区块信息（例如，“短暂上升后稳定下降”用于描述区块 5），而不离开语言模型预训练的空间。这种方法效率高，允许适应性地选择相关的源信息。

  * 为了实现这一点，我们采用了多头交叉注意力层。具体来说，对于每个头 $k=\{1, \cdots, K\}$，我们定义查询矩阵 $\mathbf{Q}_k^{(i)}=\hat{\mathbf{X}}_P^{(i)} \mathbf{W}_k^Q$，键矩阵 $\mathbf{K}_k^{(i)}=\mathbf{E}^{\prime} \mathbf{W}_k^K$ 和值矩阵 $\mathbf{V}_k^{(i)}=\mathbf{E}^{\prime} \mathbf{W}_k^V$，其中 $\mathbf{W}_k^Q \in \mathbb{R}^{d_m \times d}$ 和 $\mathbf{W}_k^K, \mathbf{W}_k^V \in \mathbb{R}^{D \times d}$。具体来说，$D$ 是骨干模型的隐藏维度，$d=\left\lfloor\frac{d_m}{K}\right\rfloor$。然后，我们定义了每个注意力头中时间序列区块重编程操作如下：
    $$
    \mathbf{Z}_k^{(i)}=\operatorname{Attention}\left(\mathbf{Q}_k^{(i)}, \mathbf{K}_k^{(i)}, \mathbf{V}_k^{(i)}\right)=\operatorname{Softmax}\left(\frac{\mathbf{Q}_k^{(i)} \mathbf{K}_k^{(i) \top}}{\sqrt{d_k}}\right) \mathbf{V}_k^{(i)} .
    $$

    通过汇总每个头中的 $\mathbf{Z}_k^{(i)} \in \mathbb{R}^{P \times d}$，我们获得 $\mathbf{Z}^{(i)} \in \mathbb{R}^{P \times d_m}$。然后，这将线性投影以与骨干模型的隐藏维度对齐，产生 $\mathbf{O}^{(i)} \in \mathbb{R}^{P \times D}$。

* 仔细理解重编程：
  * 我们将补丁嵌入重新编程到源数据表示空间，使时间序列和自然语言的模态对齐，以激活主干的时间序列理解和推理能力
  * 最笨的做法：
    * 一种常见的做法是学习一种"噪声"形式，当应用于目标输入样本时，允许预训练的源模型产生所需的目标输出，而不需要参数更新。

* Q：重编程为什么可行
  * 源数据和目标数据之间存在显式的、可学习的转换，允许对输入样本进行直接编辑。

* Q：重编程时有什么问题，挑战？
  * 时间序列既不能被直接编辑，也不能用自然语言进行无损描述，这给直接自举LLM来理解时间序列带来了巨大的挑战，而无需进行资源密集型的微调

* Q：作者是怎么做重编程的
  * 我们提出使用在骨干网络中预训练的词嵌入$\mathbf{E} \in \mathbb{R}^{V \times D}$来重新编程$\hat{\mathbf{X}}_P^{(i)}$，其中$V$是词汇表大小。
  * 然而，没有先前的知识表明哪些源标记是直接相关的。
  * 因此，简单地利用$\mathbf{E}$将导致大而可能稠密的重新编程空间。简单的解决方案是通过线性探测 $\mathbf{E}$ 来维护一个小型的文本原型集合，表示为 $\mathbf{E}^{\prime} \in \mathbb{R}^{V^{\prime} \times D}$，其中 $V^{\prime} \ll V$​。
  * 采用多头交叉注意力层来实现重编程，使得线性投影与backbone的维度对齐。
  * 然后，这将线性投影以与骨干模型的隐藏维度对齐，

* Q：重编程大致理解了什么意思了，大模型呢
  * 我们提出了一个替代性的问题：提示（prompts）是否可以作为前缀来丰富输入上下文并指导重编程时间序列区块的转换？
  * 本文将这一概念称为前缀提示( Prompt-as-Prefix，PaP )，并观察到它在补充补丁重编程(见Sec . 4.5以后)的同时，显著增强了LLM对下游任务的适应性。

* Q：怎么使用提示词的，使用大模型的过程产生了什么问题
  * ![截屏2024-01-23 21.44.19](/Users/zengyuxiang/Documents/阅读笔记/ZYX-Study-Notes/assets/截屏2024-01-23 21.44.19.png)

    这张图片看起来像是一份数据说明文档，主要内容包括对某个数据集的描述。这个数据集被称作“Electricity Transformer Temperature (ETT)”，它记录了电力长期部署中变压器油温和6个电力负载特征的数据。文档中提到了以下几个关键部分：

    1. **Domain（领域）**:
       这部分描述了数据集的应用领域。特别指出电力消耗通常在中午达到峰值，变压器负载会有显著增加。

    2. **Instruction（指令）**:
       这一部分提供了一个指令，要求预测下一个 `<H>` 步骤的数据，基于之前的 `<T>` 步骤的信息。这里 `<H>` 和 `<T>` 很可能是占位符，需要根据实际情况填入具体的步骤数。

    3. **Statistics（统计信息）**:
       在这一部分，文档描述了输入数据的统计特性，包括最小值 `<min_val>`、最大值 `<max_val>` 和中位数 `<median_val>`。还提到了数据的总体趋势是向上或向下的（`<upward or downward>`），以及最重要的五个滞后值 `<lag_val>`。

    这个说明文档可能是为了指导使用者如何处理和分析这个数据集。为了完成这项任务，使用者需要填入相应的统计数据，比如最小值、最大值等，来实现对数据的预测。这种类型的文档通常用于数据科学、机器学习项目或者其他需要数据分析的场合。

* Q：遇到什么挑战

  * 图 3(b) 展示了两种提示方法的示意图。在“区块作为前缀”（Patch-as-Prefix）方法中，一个语言模型被用来预测时间序列中的后续值，这些值用自然语言表述。这种方法遇到了一些限制：
  * （1）语言模型在没有外部工具帮助的情况下，处理高精度数字的敏感性通常会降低，因此在精确地解决长期预测任务方面面临着重大挑战；
  * （2）不同的语言模型需要复杂、定制化的后处理，因为它们是在不同的语料库上预训练的，并且可能在生成高精度数字时使用不同的分词类型，以实现精确和高效。这导致预测结果以不同的自然语言格式表示，例如['0', ', ', ' 6 ', ' 1 '] 和 [' 0 ', ' $\because$​', ' 61 ']，来表示小数 0.61。

* Q：作者使用提示词是怎么解决问题的
  * 作为前缀的提示方法巧妙地避开了这些限制。在实际操作中，我们确定了构建有效提示的三个关键组成部分：
    * （1）数据集上下文，
    * （2）任务指令和
    * （3）输入统计信息。
  * 提示示例如图4所示。数据集上下文为语言模型提供了有关输入时间序列的重要背景信息，这些时间序列通常在不同领域之间表现出不同的特点。任务指令在将补丁嵌入转换为特定任务时起到了关键的指导作用。我们还通过添加趋势和滞后等额外的关键统计信息来丰富输入时间序列，以促进模式识别和推理。

* Q：最后一步是怎么做的
  * **Output Projection.** Upon packing and feedforwarding the prompt and patch embeddings $\mathbf{O}^{(i)}$ through the frozen LLM as shown in Eig._2, we discard the prefixal part and obtain the output representations. Following this, we flatten and linear project them to derive the final forecasts $\hat{\mathbf{Y}}^{(i)}$.

* Q：基础问题，什么是patch embedding

  在时序预测中，"patch embedding" 是一种将时间序列数据划分成小块（或称为"patch"）并将这些小块转化为向量表示的技术。这通常用于处理长时间序列，以便更好地捕获序列中的局部特征和模式。

  具体来说，patch embedding 过程包括以下步骤：

  1. 将时间序列划分成若干个连续的、固定长度的子序列块，这些块通常称为"patch"。
  2. 对每个 patch 内的数据进行处理，通常是将其转化为一个固定维度的向量。这可以通过一些方法来实现，如卷积神经网络（CNN）或自注意力机制（如Transformer模型）等。
  3. 这些转化后的向量可以被输入到机器学习模型（如循环神经网络（RNN）或Transformer等）中，以用于进一步的时间序列预测任务。

  使用 patch embedding 可以帮助模型更好地理解和处理长时间序列数据，因为它允许模型聚焦于局部模式和特征，而不必处理整个时间序列的信息。这在许多时间序列预测任务中都是有用的，如股票价格预测、天气预测等。

然而，时间序列既不能被直接编辑，也不能用自然语言进行无损描述，这给直接自举LLM来理解时间序列带来了巨大的挑战，而无需进行资源密集型的微调。