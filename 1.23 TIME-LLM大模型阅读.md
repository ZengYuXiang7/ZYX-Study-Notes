# 1.23 TIME-LLM大模型论文阅读

### TIME-LLM: TIME SERIES FORECASTING BY REPROGRAMMING LARGE LANGUAGE MODELS



* Q：什么领域
  * 时间序列预测在许多现实世界的动态系统中具有重要意义，并得到了广泛的研究。与自然语言处理( NLP )和计算机视觉( CV )不同，单个大型模型可以处理多个任务，用于时间序列预测的模型通常是专门的，这就需要为不同的任务和应用设计不同的模型。
* Q：解决了什么挑战（简单）
  * 挑战仍然在于有效地对时间序列数据和自然语言的模态进行对齐，以利用这些能力。
* Q：做了啥
  * 一个重编程框架，将LLM重用于一般的时间序列预测，而主干语言模型保持完整。
    * 我们首先将输入的时间序列与文本原型重新编程，然后将其输入到冻结的LLM中，使两种模态对齐。
  * 为了增强LLM对时间序列数据的推理能力，我们提出了前缀提示( Prompt-as-Prefix，PaP )，它丰富了输入上下文，指导了重新编程的输入补丁的转换。
  * 最后，将LLM变换后的时间序列块进行投影，得到预测结果。
* Q：解决了何种挑战问题
  * 虽然时间序列建模并没有从同样的重大突破中受益，但LLMs令人印象深刻的能力启发了它们在时间序列预测中的应用。
  * 现有的非LLM方法在很大程度上是统计性的，没有太多的先天推理。
  * 因为LLMs操作的是离散的令牌，而时间序列数据本质上是连续的。此外，用于解释时间序列模式的知识和推理能力并不自然地存在于LLMs的预训练中。
* Q：核心贡献
  * 其核心思想是将输入的时间序列重新编程为更自然地适合语言模型能力的文本原型表示。
  * 目标是重新编程一个大的语言模型f ( · )来理解输入的时间序列，并准确地预测H个未来时间步的读数。
* Q：大模型微调是什么
  * 只更新了轻量级输入转换和输出投影的参数，而主干语言模型是冻结的。
* Q：微调的好处是
  * TIME - LLM是直接优化的，只需要少量的时间序列和少量的训练历元就可以获得，与从头开始构建大型特定领域模型或对其进行微调相比，保持了较高的效率并施加了较少的资源约束。

* Q：无需对骨干模型进行任何微调。如何理解

  "Backbone model"在不同的上下文中可能有不同的含义，但在机器学习和深度学习的领域，它通常指的是一个用于特定任务（如图像识别、自然语言处理等）的基础网络结构。这个基础网络（backbone）作为一个核心组件，被用于提取特征，然后可以在其上叠加其他层或模块来完成更复杂的任务。

  在许多深度学习应用中，一个强大的骨干网络是至关重要的，因为它能够从原始数据中有效地提取有用的特征。例如，在图像识别中，常用的骨干网络包括ResNet、VGG、Inception等。这些网络经过大量数据的预训练，能够捕捉到图像中的关键信息，如边缘、纹理和形状。

  "无需对骨干模型进行任何微调"通常意味着在特定应用中直接使用预训练的骨干模型，而不需要对其进行进一步的训练或调整。这在一些情况下是可行的，特别是当预训练模型已经能够很好地捕捉到对当前任务来说重要的特征时。使用未经微调的预训练模型可以节省大量的时间和计算资源，尤其是在资源有限的情况下。然而，这种方法的效果很大程度上取决于预训练模型的质量和与当前任务的相关性。

  骨干模型：backbone model，代指

* 重编程原文

  * 为了弥补这一差距，我们提出利用预训练的词嵌入 $\mathbf{E} \in \mathbb{R}^{V \times D}$ 对骨干模型中的 $\hat{\mathbf{X}}_P^{(i)}$ 进行重编程，其中 $V$ 是词汇量。然而，并没有先验知识指出哪些源词汇是直接相关的。因此，仅仅利用 $\mathbf{E}$ 将导致一个庞大且可能是密集的重编程空间。一个简单的解决方案是通过线性探测 $\mathbf{E}$ 来维护一个小型的文本原型集合，表示为 $\mathbf{E}^{\prime} \in \mathbb{R}^{V^{\prime} \times D}$，其中 $V^{\prime} \ll V$。如图 3 (a) 所示。文本原型学习连接语言提示，例如“长稳定”（蓝线）和“短上升”（红线），然后结合起来表示局部区块信息（例如，“短暂上升后稳定下降”用于描述区块 5），而不离开语言模型预训练的空间。这种方法效率高，允许适应性地选择相关的源信息。为了实现这一点，我们采用了多头交叉注意力层。具体来说，对于每个头 $k=\{1, \cdots, K\}$，我们定义查询矩阵 $\mathbf{Q}_k^{(i)}=\hat{\mathbf{X}}_P^{(i)} \mathbf{W}_k^Q$，键矩阵 $\mathbf{K}_k^{(i)}=\mathbf{E}^{\prime} \mathbf{W}_k^K$ 和值矩阵 $\mathbf{V}_k^{(i)}=\mathbf{E}^{\prime} \mathbf{W}_k^V$，其中 $\mathbf{W}_k^Q \in \mathbb{R}^{d_m \times d}$ 和 $\mathbf{W}_k^K, \mathbf{W}_k^V \in \mathbb{R}^{D \times d}$。具体来说，$D$ 是骨干模型的隐藏维度，$d=\left\lfloor\frac{d_m}{K}\right\rfloor$。然后，我们定义了每个注意力头中时间序列区块重编程操作如下：
    $$
    \mathbf{Z}_k^{(i)}=\operatorname{Attention}\left(\mathbf{Q}_k^{(i)}, \mathbf{K}_k^{(i)}, \mathbf{V}_k^{(i)}\right)=\operatorname{Softmax}\left(\frac{\mathbf{Q}_k^{(i)} \mathbf{K}_k^{(i) \top}}{\sqrt{d_k}}\right) \mathbf{V}_k^{(i)} .
    $$

    通过汇总每个头中的 $\mathbf{Z}_k^{(i)} \in \mathbb{R}^{P \times d}$，我们获得 $\mathbf{Z}^{(i)} \in \mathbb{R}^{P \times d_m}$。然后，这将线性投影以与骨干模型的隐藏维度对齐，产生 $\mathbf{O}^{(i)} \in \mathbb{R}^{P \times D}$。

* 仔细理解重编程：
  * 我们将补丁嵌入重新编程到源数据表示空间，使时间序列和自然语言的模态对齐，以激活主干的时间序列理解和推理能力
  * 最笨的做法：
    * 一种常见的做法是学习一种"噪声"形式，当应用于目标输入样本时，允许预训练的源模型产生所需的目标输出，而不需要参数更新。
* Q：重编程为什么可行
  * 源数据和目标数据之间存在显式的、可学习的转换，允许对输入样本进行直接编辑。
* Q：重编程时有什么问题，挑战？
  * 时间序列既不能被直接编辑，也不能用自然语言进行无损描述，这给直接自举LLM来理解时间序列带来了巨大的挑战，而无需进行资源密集型的微调
* Q：作者是怎么做重编程的
  * 我们提出使用在骨干网络中预训练的词嵌入$\mathbf{E} \in \mathbb{R}^{V \times D}$来重新编程$\hat{\mathbf{X}}_P^{(i)}$，其中$V$是词汇表大小。
  * 然而，没有先前的知识表明哪些源标记是直接相关的。
  * 因此，简单地利用$\mathbf{E}$将导致大而可能稠密的重新编程空间。简单的解决方案是通过线性探测 $\mathbf{E}$ 来维护一个小型的文本原型集合，表示为 $\mathbf{E}^{\prime} \in \mathbb{R}^{V^{\prime} \times D}$，其中 $V^{\prime} \ll V$。
* 
* Q：



然而，时间序列既不能被直接编辑，也不能用自然语言进行无损描述，这给直接自举LLM来理解时间序列带来了巨大的挑战，而无需进行资源密集型的微调。