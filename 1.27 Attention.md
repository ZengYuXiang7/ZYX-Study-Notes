# 1.27 Attention

* Q：自注意力Self Attention机制

  - 查询 (Query) (Q) : 查询是当前位置或当前词嵌入在自注意力机制中的表示。它用于衡量当前位置与其他位置之间的相关性，从而确定在生成输出时应该关注哪些信息。

  - 键 (Keys) (K)：键是序列中所有位置的表示。它们用于计算当前位置与其他位置之间的相关性得分。键帮助模型了解当前位置与序列中其他位置之间的关系

  - 值 (Values) ( $\mathrm{v})$ ：值是与键相对应的信息。在自注意力机制中，值用于生成最终的输出，它们的权重由查询和键的相关性决定。



* Q * K 到底是什么

  * 这个点积操作实际上是在计算序列中每个元素与其他元素之间的关系强度。点积的结果是一个注意力分数矩阵，其中每个元素表示一个特定的Query与一个特定的Key之间的匹配程度。

* Q * K为什么是相关性

  内积（点积）被用作衡量相关性的一个工具，其背后的原因主要涉及数学和几何上的含义。在自注意力机制中，使用内积来计算Query和Key之间的相关性是基于以下几个方面：

  1. **几何解释**：在几何上，两个向量的内积反映了它们之间的相似性。当两个向量的方向相近时，它们的内积较大；当它们垂直时，内积为零；当它们方向相反时，内积为负。这意味着内积可以作为衡量两个向量方向相似程度的一种手段。

  2. **余弦相似度**：内积与余弦相似度密切相关。两个向量的内积除以它们的模长乘积，就得到了余弦相似度。余弦相似度度量的是向量在方向上的相似程度，而方向上的相似性正是我们在衡量相关性时关心的重点。

  3. **投影**：内积在数学上可以被理解为一个向量在另一个向量上的投影。在自注意力机制中，将Query向量在Key向量上的投影作为它们之间相关性的度量，实际上是在计算Query在Key所代表的信息方向上的成分有多大。

  4. **高效计算**：在实践中，使用内积来计算向量之间的相关性是计算上非常高效的。这在处理大型数据集和复杂模型时尤其重要。

  综上所述，在自注意力机制中，使用Query和Key之间的内积来计算它们的相关性是一种数学上合理且计算上高效的方法。它通过衡量Query在Key所在方向上的投影大小，来反映它们之间的相似程度，从而确定注意力分数。



当然，下面是自注意力（Attention）机制的基本数学公式。这些公式描述了如何计算注意力分数以及如何将这些分数应用于输入数据。

假设我们有输入序列 \( X \)，其中每个元素经过线性变换生成了三个向量：Query \( Q \)，Key \( K \)，Value \( V \)。这些向量通常通过不同的权重矩阵从输入数据中得到：

\[ $Q = XW^Q$ \]
\[ $K = XW^K$ \]
\[ $V = XW^V$ \]

其中，\( $W^Q $\)，\( $W^K$ \)，\( $W^V$ \) 是模型学习到的权重矩阵。

接下来，自注意力机制通过以下步骤计算输出：

1. **计算注意力分数**：首先计算Query和Key的点积，然后除以 \( $\sqrt{d_k} $\) 进行缩放（\( $d_k$ \) 是Key向量的维度）。

   \[ $\text{Attention Score} = \frac{QK^T}{\sqrt{d_k}}$ \]

2. **应用softmax函数**：对注意力分数应用softmax函数进行规范化，以便每一行的分数加起来为1。

   \[ $\text{Softmax Score} = \text{softmax}(\text{Attention Score}) $\]

3. **应用到Value**：将这些规范化的注意力分数应用于Value。

   \[ $\text{Output} = \text{Softmax Score} \times V$ \]

最终，输出是Value的加权组合，其中的权重由输入的Query和Key之间的关系决定。这种机制允许模型在处理每个输入元素时动态地关注（即“注意”）输入序列中的其他元素。这在自然语言处理和序列分析任务中尤其有效，因为它允许模型捕获输入数据中的长距离依赖关系。