# 1.26 CLSR时序解藕

### Disentangling Long and Short-Term Interests for Recommendation

* Q：为了什么而做
  * 建模用户的长期和短期兴趣对于准确推荐至关重要。
  * 然而，由于没有人工标注的用户兴趣标签，现有的方法总是遵循将这两个方面纠缠在一起的范式，这可能导致推荐准确性和可解释性较差。在本文中
* Q：做了什么
  * 我们首先提出了两个独立的编码器来独立地捕获不同时间尺度的用户兴趣。
  * 然后，我们从交互序列中提取长期和短期兴趣代理，作为用户兴趣的伪标签。
  * 然后设计成对对比任务来监督兴趣表征与其对应的兴趣代理之间的相似性。
  * 最后，由于长期和短期利益的重要性是动态变化的，我们提出通过基于注意力的网络自适应地将它们聚合在一起进行预测。
* Q：解决了什么挑战
  * 具体来说，在这些方法中，基于CF的模型如矩阵分解用于长期利益，而序列模型用于学习短期利益。然而，LS - term兴趣是否能够被相应的模型有效地捕获是没有保证的，因为它们没有对学习到的LS - term兴趣施加明确的监督。换句话说，这些方法中学习到的LS -项兴趣可以相互纠缠[ 28 ]。
    * LS - term兴趣反映了用户偏好的不同方面。具体来说，长期兴趣可以看作是用户的整体偏好，能够在较长时间内保持稳定，而短期兴趣则表示用户的动态偏好，能够根据最近的交互快速演化。因此，学习LS - term兴趣度的统一表示不足以捕捉到这种差异。相反，对这两个方面分别建模更为合适。
    * 为了学习LS - term兴趣，很难获得有标签的数据。采集到的行为日志数据往往只包含用户的点击等隐式反馈。因此，LS -长期利益的单独建模对于区分这两个方面缺乏明确的监督。
    * 对于用户未来交互行为的最终预测，应该同时考虑长期和短期利益。然而，在不同的用户-项目交互中，两种兴趣的重要性是不同的。例如，当用户持续浏览相似的项目时，其短期兴趣更为重要，而当用户切换到差异较大的项目时，其行为在很大程度上受到长期兴趣的驱动。因此，自适应地融合这两个方面对于预测未来的交互是关键但具有挑战性的。
* Q：作者是如何解决的（核心贡献）
  * 我们提出了一个对比学习框架，该框架利用交互序列来分离LS项的兴趣，从而构建自我监督信号。具体来说，为了独立地捕获LS -长期利益，我们提出将每个交互分解为三个机制：长期利益表示、短期利益演化和交互预测。
  * 我们设计了两个动态随时间变化的独立编码器，分别对LS - term兴趣进行建模，解决了第一个挑战。
  * 为了克服LS - term兴趣缺乏标记数据的关键挑战，我们提出使用自我监督[ 7 ]。
  * 我们首先通过分别提取用户的全部历史交互和近期交互来生成长期/短期兴趣的代理表示。
  * 然后，我们以对比的方式监督从两个独立的编码器中获得的兴趣表示与其对应的代理比相反的代理更相似
  * 与现有方法没有对学习到的LS项兴趣度[ 2、47 ]施加明确的监督不同，我们的自监督方法可以学习到对LS项兴趣度更好的分离表示，并消除对标记数据的依赖
  * 利用解缠后的兴趣表示，我们设计了一个基于注意力的融合网络，自适应地聚合两个方面进行预测，解决了最后一个挑战。

* Q：问题是如何定义的

  符号说明。用 $M$ 表示用户数量，用 $\{\boldsymbol{x}^{\boldsymbol{u}}\}_{u=1}^M$ 表示所有用户的交互序列。每个序列 $\boldsymbol{x}^{\boldsymbol{u}} = \left[x_1^u, x_2^u, \ldots, x_{T_u}^u\right]$ 代表一个按照交互时间戳排序的物品列表。这里的 $T_u$ 表示用户 $u$ 的交互历史长度，每个物品 $x_t^u$ 在 $[1, N]$ 范围内，其中 $N$ 表示物品的数量。

  由于用户的交互历史 $\boldsymbol{x}^{\boldsymbol{u}}$ 反映了长期和短期兴趣，推荐系统将首先从 $\boldsymbol{x}^{\boldsymbol{u}}$ 学习长短期兴趣，然后基于这两个方面预测未来的交互。因此，我们可以将学习推荐中的长短期兴趣的问题定义如下：

   输入：所有用户的历史交互序列 ${\boldsymbol{x}^{\boldsymbol{u}}}_{u=1}^M$。

   输出：一个预测模型，用于估计用户点击某个物品的概率，同时考虑长短期兴趣。

* 这篇论文乍一看还是很好理解的

* Q：用户兴趣是怎么建模的？（先看看原文）
  $$
  \zeta=\left\{\begin{array}{l}
  U_l=f_1(U) \\
  U_s^{(t)}=f_2\left(U_s^{(t-1)}, V^{(t-1)}, Y^{(t-1)}, U\right), \\
  Y^{(t)}=f_3\left(U_l, U_s^{(t)}, V^{(t)}, U\right)
  \end{array}\right.
  $$
  where $f_1, f_2$ and $f_3$ are the underlying functions for user $U$ 's longterm interests $\left(U_l\right)$, short term interests $\left(U_s^{(t)}\right)$ and interaction $\left(Y^{(t)}\right)$ with item $V^{(t)}$. Current and last timestamps are denoted as $t$ and $t-1$, respectively. It is worthwhile noting that $U$ denotes user profile, which contains the user ID and the interaction history $\boldsymbol{x}^{\boldsymbol{u}}$​​.

  提出的用户兴趣建模ζ将每个交互分解为三种机制：$f_1$长期兴趣表示、$f_2$短期兴趣演化和$f_3$交互预测，并在图1中简要说明。我们现在对这三种机制的细节进行解释。

* Q：不同时期的兴趣怎么定义

  * 长期兴趣的表征（方程式（1）中）。长期兴趣反映了用户偏好的整体视角，因此它是稳定的，不太受近期交互的影响。换句话说，长期兴趣可以从整个历史交互序列中推断出来，因此我们将 $U$ 作为 $f_1$ 的输入，其中包含了交互历史 $\boldsymbol{x}^{\boldsymbol{u}}$。
  * 短期兴趣的演变（方程式（2）中）。短期兴趣随着用户不断与推荐物品交互而演变。例如，用户在点击某个物品后可能会形成新的兴趣。同时，用户也可能逐渐失去某些兴趣。也就是说，短期兴趣是时变变量，因此在 $f_2$ 中，时刻 $t$ 的短期兴趣 $U_s^{(t)}$ 是从时刻 $t-1$ 的短期兴趣 $U_s^{(t-1)}$ 递归演变的，受到上一次与物品 $V^{(t-1)}$ 的交互 $Y^{t-1}$ 的影响。
  * 交互预测（方程式（3）中）。在预测未来交互时，长期兴趣和短期兴趣哪个更重要取决于多种因素，包括目标物品 $V^{(t)}$ 和用户 $U$ 的交互历史 $\boldsymbol{x}^{\boldsymbol{u}}$。因此，我们根据 $V^{(t)}$ 和 $U$ 以一种自适应的方式融合 $U_l$ 和 $U_s^{(t)}$ 来准确预测交互。

* Q：作者是怎么解藕用户长期兴趣，短期兴趣

  * 对LS -长期利益的拆解意味着，$U_1$只捕获长期利益，而$U_s$​则模拟纯粹的短期利益。这样的解缠有助于实现可解释和可控的推荐，因为我们可以通过调整融合权重来跟踪和调整每个方面的重要性。同时，LS - term兴趣的有效调整要求学习到的表示只包含期望方面的信息。以线性情形为例，假设一个推荐模型纠缠LS -项兴趣如下
    $$
    U_l^{\prime}=0.6 U_l+0.4 U_s, U_s^{\prime}=0.4 U_l+0.6 U_s,
    $$
    where $U_l^{\prime}$ and $U_s^{\prime}$ are the learned entangled interests. Given the fusion weights (importance) of LS-term interests as 0.8 and 0.2 respectively, the actual fused interests are computed as follows,
    $$
    U_{\text {fuse }}^{\prime}=0.8 U_l^{\prime}+0.2 U_s^{\prime}=0.56 U_l+0.44 U_s,
    $$

* Q：那么，如何学习长期兴趣和短期
  * 然而，由于Ul和Us没有带标签的数据，分离LS - term兴趣具有挑战性。我们现在详细介绍我们的对比学习框架，它可以实现与自我监督的强解纠缠。
  * 作者提出了对比学习框架

*****

具体实现步骤

* Q：作者是怎么为对比学习做前置准备的（以下为原文）

  3.2.1 生成长短期兴趣查询向量。

  ​	受到最近研究 [2,29,47,48] 的启发，这些研究分别使用两种不同模型来学习长短期兴趣，我们设计了两个独立的注意力编码器，$\phi$ 和 $\psi$，分别捕捉这两方面的兴趣。首先，我们按以下方式生成长短期兴趣查询向量，
  $$
  \begin{aligned}
  & \boldsymbol{q}_{\boldsymbol{l}}^{\boldsymbol{u}}=\operatorname{Embed}(u), \\
  & \boldsymbol{q}_s^{\boldsymbol{u}, \boldsymbol{t}}=\operatorname{GRU}\left(\left\{x_1^u, \cdots, x_t^u\right\}\right),
  \end{aligned}
  $$
  其中我们使用一个查找嵌入表和一个门控循环单元（GRU）[9] 来捕捉随时间变化的不同动态。为了在嵌入相似性上施加额外的自监督，所有的嵌入都需要处于同一个语义空间中。因此，我们将历史物品序列作为注意力编码器的键，从而获得的长短期兴趣表征位于与物品嵌入相同的空间，如下所示，
  $$
  \begin{aligned}
  \boldsymbol{u}_{\boldsymbol{l}}^{\boldsymbol{t}} & =\phi\left(\boldsymbol{q}_{\boldsymbol{l}}^{\boldsymbol{u}},\left\{x_1^u, \cdots, x_t^u\right\}\right), \\
  \boldsymbol{u}_{\boldsymbol{s}}^{\boldsymbol{t}} & =\psi\left(\boldsymbol{q}_s^{\boldsymbol{u}, \boldsymbol{t}},\left\{x_1^u, \cdots, x_t^u\right\}\right),
  \end{aligned}
  $$
  其中 $\boldsymbol{u}_l^t$ 和 $\boldsymbol{u}_s^t$ 是学习到的长短期兴趣的表征。我们现在介绍提出的长短期兴趣编码器。

  3.2.2 长期兴趣编码器。

  ​	图 2 (B) 展示了提出的长期兴趣编码器 $\phi$。我们使用注意力池化来提取长期兴趣表征，每个物品 $x_j^u$ 的注意力得分可以按以下方式计算，
  $$
  \begin{aligned}
  \boldsymbol{v}_j & =W_{\boldsymbol{l}} \boldsymbol{E}\left(x_j^u\right), \\
  \alpha_j & =\tau_l\left(\boldsymbol{v}_{\boldsymbol{j}}\left\|\boldsymbol{q}_{\boldsymbol{l}}^{\boldsymbol{u}}\right\|\left(\boldsymbol{v}_{\boldsymbol{j}}-\boldsymbol{q}_{\boldsymbol{l}}^{\boldsymbol{u}}\right) \|\left(\boldsymbol{v}_{\boldsymbol{j}} \cdot \boldsymbol{q}_{\boldsymbol{l}}^{\boldsymbol{u}}\right)\right), \\
  a_j & =\frac{\exp \left(\alpha_j\right)}{\sum_{i=1}^t \exp \left(\alpha_i\right)},
  \end{aligned}
  $$
  其中 $W_l$ 是一个转换矩阵，$\tau_l$ 是一个多层感知机（MLP）网络，$\|$ 表示嵌入的连接。最终学到的长期兴趣表征是整个交互历史的加权聚合，权重通过上述

  注意力网络计算，公式如下，
  $$
  \boldsymbol{u}_l^{\boldsymbol{t}}=\sum_{j=1}^t a_j \cdot \boldsymbol{E}\left(x_j^u\right)
  $$
  3.2.3 短期兴趣编码器。

  ​	用户交互的序列模式在短期兴趣建模中扮演关键角色，因此我们在一个递归神经网络（RNN）之上使用另一个注意力网络。具体来说，我们将历史物品嵌入输入到一个 RNN 模型，并使用 RNN 的输出作为键，公式如下，
  $$
  \begin{aligned}
  & \left\{\boldsymbol{o}_1^{\boldsymbol{u}}, \ldots, \boldsymbol{o}_{\boldsymbol{t}}^{\boldsymbol{u}}\right\}=\rho\left(\left\{\boldsymbol{E}\left(x_1^u\right), \ldots, \boldsymbol{E}\left(x_t^u\right)\right\}\right), \\
  & \boldsymbol{v}_{\boldsymbol{j}}=\boldsymbol{W}_{\boldsymbol{s}} \boldsymbol{o}_{\boldsymbol{j}}^{\boldsymbol{u}},
  \end{aligned}
  $$
  其中 $\boldsymbol{W}_{\boldsymbol{s}}$ 是一个转换矩阵，$\rho$ 代表一个 RNN 模型。在第 4 节，我们将通过实验评估 RNN 模型的不同实现，包括 LSTM [18]、GRU [9] 和 Time4LSTM [47]。与方程式 (18) 和 (19) 类似，我们使用 $\boldsymbol{q}_s^{\boldsymbol{u}, \boldsymbol{t}}$ 作为查询向量，并获得注意力得分 $b_k$。然后可以计算短期兴趣的学习表征，
  $$
  \boldsymbol{u}_s^{\boldsymbol{t}}=\sum_{j=1}^t b_j \cdot \boldsymbol{o}_j^u .
  $$

  ​	尽管采用了独立的编码器，但由于 $\boldsymbol{u}_{\boldsymbol{l}}^{\boldsymbol{t}}$ 和 $\boldsymbol{u}_s^{\boldsymbol{t}}$ 是以无监督的方式提取的，因此不能保证长短期兴趣的分离 [28]。特别是，没有标签数据来监督学习到的兴趣表征。因此，我们提出设计对比任务，通过自监督实现分离，并克服缺乏标签数据的挑战。



os：

* Q：先不看对比学习关键部分，看看prediction layer

  3.2.5 交互预测的自适应融合。

  ​	通过自监督学习得到的解耦表征后，如何聚合这两个方面来预测交互仍然是一个挑战。简单的聚合方法，如求和和连接，假设长短期兴趣的贡献是固定的，这在许多情况下是不成立的。事实上，长期兴趣和短期兴趣哪个更重要取决于历史序列。例如，当用户连续浏览同一类别的物品时，他们主要受短期兴趣的驱动。同时，长短期兴趣的重要性也取决于目标物品。例如，一个喜爱运动的用户可能仍然因为长期兴趣点击推荐的自行车，即使他/她刚刚浏览了几本书。因此，我们将历史序列和目标物品作为聚合器的输入，其中历史序列使用 GRU 进行压缩。图 2 (D) 展示了提出的基于注意力的自适应融合模型，它动态地确定长短期兴趣的重要性，以融合 $\boldsymbol{u}_l^t$ 和 $\boldsymbol{u}_s^{\boldsymbol{t}}$。具体来说，最终融合的兴趣表征如下所得，
  $$
  \begin{aligned}
  & \boldsymbol{h}_t^{\boldsymbol{u}}=\operatorname{GRU}\left(\left\{\boldsymbol{E}\left(x_1^u\right), \ldots, \boldsymbol{E}\left(x_t^u\right)\right\}\right), \\
  & \alpha=\sigma\left(\tau_f\left(\boldsymbol{h}_t^u\left\|\boldsymbol{E}\left(x_{t+1}^u\right)\right\| \boldsymbol{u}_{\boldsymbol{l}}^{\boldsymbol{t}} \| \boldsymbol{u}_s^{\boldsymbol{t}}\right),\right. \\
  & \boldsymbol{u}^t=\alpha \cdot \boldsymbol{u}_{\boldsymbol{l}}^{\boldsymbol{t}}+(1-\alpha) \cdot \boldsymbol{u}_s^{\boldsymbol{t}}
  \end{aligned}
  $$
  其中 $\sigma$ 是 sigmoid 激活函数，$\tau_f$ 是用于融合的多层感知机（MLP）。这里的 $\alpha$ 表示基于历史交互、目标物品和用户长短期兴趣估算的融合权重。

  ​	为了预测交互，我们使用广泛采用的两层 MLP [47]，如图 2 (E) 所示。然后，给定用户 $u$ 和物品 $v$ 在时刻 $t+1$ 的预测分数可以如下计算，
  $$
  \hat{y}_{u, v}^{t+1}=\operatorname{MLP}\left(\boldsymbol{u}^t \| \boldsymbol{E}(v)\right) .
  $$

* Q：如何训练：推荐系统常见损失⬇️

  根据现有工作的设置 [47]，我们使用负对数似然损失函数如下，
  $$
  \mathcal{L}_{\text {rec }}^{u, t}=-\frac{1}{N} \sum_{v \in O} y_{u, v}^{t+1} \log \left(\hat{y}_{u, v}^{t+1}\right)+\left(1-y_{u, v}^{t+1}\right) \log \left(1-\hat{y}_{u, v}^{t+1}\right)
  $$
  其中 $O$ 是由

  ​	一个正样本物品 $x_{t+1}^u$ 和 $N-1$ 个采样负样本物品组成的集合。我们以端到端的方式训练模型，采用多任务学习的两个目标。具体来说，联合损失函数带有一个用于平衡目标的超参数 $\beta$，可以如下公式化，
  $$
  \mathcal{L}=\sum_{u=1}^M \sum_{t=1}^{T_u}\left(\mathcal{L}_{\text {rec }}^{u, t}+\beta \mathcal{L}_{\text {con }}^{u, t}\right)+\lambda\|\Theta\|_2,
  $$
  其中 $\lambda\|\Theta\|_2$ 表示用于解决过拟合的 $L2$ 正则化。我们实现的计算复杂度是 $\mathcal{O}(\mathrm{M}+$ $N) d+Q)$，其中 $Q$ 表示 MLP 和 GRU 的复杂度，与最先进的 SLi-Rec 方法 [47] 相当。

* 他的对比学习是怎么学习的（看原文）

  ​	如前所述，长期兴趣提供了用户偏好的整体观，它总结了整个历史交互，而短期兴趣随时间动态演化，它反映了最近的交互。因此，我们可以得到代理变量对于LS - term利益从交互序列本身来监督两个利益编码器。具体来说，我们计算了整个互动历史的平均表示作为长期利益的代理，并使用最近k次互动的平均表示作为短期利益的代理。形式上，给定用户u在时间戳t的LS -长期利益代理可以计算如下：
  $$
  \begin{aligned}
  & \boldsymbol{p}_{\boldsymbol{l}}^{\boldsymbol{u}, \boldsymbol{t}}=\mathbf{M E A N}\left(\left\{x_1^u, \cdots, x_t^u\right\}\right)=\frac{1}{t} \sum_{j=1}^t \boldsymbol{E}\left(x_j^u\right), \\
  & \boldsymbol{p}_s^{u, t}=\mathbf{MEAN}\left(\left\{x_{t-k+1}^u, \cdots, x_t^u\right\}\right)=\frac{1}{k} \sum_{j=1}^k \boldsymbol{E}\left(x_{t-j+1}^u\right),
  \end{aligned}
  $$
  ​	其中 $E(x)$ 表示物品 $x$ 的嵌入。注意，我们只在序列长度超过某个阈值 $l_t$ 时才计算代理，因为如果整个序列只包含少量物品，那么没有必要区分长期和短期兴趣。这个阈值 $l_t$ 以及最近行为序列的长度 $k$ 是我们方法中的超参数。此外，我们在这里使用平均池化，因为它简单且性能表现足够好。实际上，我们的自监督范式能够利用更复杂的设计来处理代理，这将是我们未来工作的一部分。

  ​	通过自监督学习得到的解耦表征后，如何聚合这两个方面来预测交互仍然是一个挑战。当序列长度超过阈值 $l_t$ 时，我们才会计算代理（proxies），因为如果整个序列只包含少量物品，那么没有必要区分长短期兴趣。这个阈值 $l_t$，以及最近行为序列的长度 $k$，是我们方法中的超参数。此外，我们在这里使用平均池化，因为它简单且性能已经足够好。实际上，我们的自监督范式能够利用更复杂的代理设计，这我们留待未来的工作。

  ​	以代理作为标签，我们可以利用它们来监督长短期兴趣的解耦。具体来说，我们在编码器输出和代理之间进行对比学习，这要求学习到的长短期兴趣表征与它们对应的代理比起相反的代理更相似。我们在图 2 (A) 中展示了对比任务。具体来说，有四个对比任务如下，
  $$
  \begin{aligned}
  & \operatorname{sim}\left(\boldsymbol{u}_l^t, \boldsymbol{p}_l^{\boldsymbol{u}, t}\right)>\operatorname{sim}\left(\boldsymbol{u}_l^t, \boldsymbol{p}_s^{\boldsymbol{u}, \boldsymbol{t}}\right), \\
  & \operatorname{sim}\left(\boldsymbol{p}_l^{\boldsymbol{u}, t}, \boldsymbol{u}_l^t\right)>\operatorname{sim}\left(\boldsymbol{p}_l^{\boldsymbol{u}, t}, \boldsymbol{u}_s^t\right), \\
  & \operatorname{sim}\left(\boldsymbol{u}_s^t, \boldsymbol{p}_s^{\boldsymbol{u}, t}\right)>\operatorname{sim}\left(\boldsymbol{u}_s^t, \boldsymbol{p}_l^{\boldsymbol{u}, t}\right), \\
  & \operatorname{sim}\left(\boldsymbol{p}_s^{\boldsymbol{u}, t}, \boldsymbol{u}_s^t\right)>\operatorname{sim}\left(\boldsymbol{p}_s^{\boldsymbol{u}, t}, \boldsymbol{u}_l^t\right),
  \end{aligned}
  $$
  其中方程式 (19)-(20) 监督长期兴趣，方程式 (21)-(22) 监督短期兴趣，$\operatorname{sim}(\cdot, \cdot)$ 衡量嵌入相似性。以长期兴趣建模为例，方程式 (19) 鼓励学到的长期兴趣表征 $\boldsymbol{u}_l^t$ 与长期代理 $p_l^{u, t}$ 相比短期代理 $\boldsymbol{p}_s^{\boldsymbol{u}, \boldsymbol{t}}$ 更相似。同时，方程式 (20) 要求 $\boldsymbol{u}_{\boldsymbol{l}}^{\boldsymbol{t}}$ 与 $\boldsymbol{p}_l^{\boldsymbol{u}, \boldsymbol{t}}$ 相比短期兴趣表征 $\boldsymbol{u}_s^{\boldsymbol{t}}$ 更接近。通过对编码器输出和代理之间相似性的四个对称对比任务，我们在长短期兴趣建模中增加了自监督，与现有的无监督方法相比，可以实现更强的解耦。

  ​	我们根据贝叶斯个性化排序 (BPR) [35] 和三元损失实现了两种成对损失函数，以完成方程式 (19)-(22) 中的对比学习。具体来说，这两个损失函数使用内积和欧几里得距离来捕捉嵌入相似性，计算如下，
  $$
  \begin{aligned}
  & \mathcal{L}_{\text {bpr }}(a, p, q)=\sigma(\langle a, q\rangle-\langle a, p\rangle) \\
  & \mathcal{L}_{\text {tri }}(a, p, q)=\max \{d(a, p)-d(a, q)+m, 0\}
  \end{aligned}
  $$
  其中 $\sigma$ 是 softplus 激活函数，$\langle\cdot, \cdot\rangle$ 表示两个嵌入的内积，$d$ 表示欧几里得距离，$m$ 表示正的边际值。$\mathcal{L}_{\text {bpr}}$ 和 $\mathcal{L}_{\text {tri}}$ 都旨在使锚点 $a$ 与正样本 $p$ 相比负样本 $q$ 更相似。因此，用于自监督解耦长短期兴趣的对比损失可以如下计算，
  $$
  \mathcal{L}_{\text {con }}^{u, t}=f\left(\boldsymbol{u}_l, \boldsymbol{p}_l, \boldsymbol{p}_s\right)+f\left(\boldsymbol{p}_l, \boldsymbol{u}_l, \boldsymbol{u}_s\right)+f\left(\boldsymbol{u}_s, \boldsymbol{p}_s, \boldsymbol{p}_l\right)+f\left(\boldsymbol{p}_s, \boldsymbol{u}_s, \boldsymbol{u}_l\right)
  $$
  其中我们省略了兴趣表征和代理的上标，$f$ 可以是 $\mathcal{L}_{\text {bpr}}$ 或 $\mathcal{L}_{\text {tri}}$。

  ​	备注。用户的LS - term兴趣也可以在一定程度上相互重叠。例如，在电子商务应用上只购买衣服的用户往往具有一致的LS - term兴趣。因此，与现有的去纠缠推荐方法[ 43、49 ]不同，[ 43、49 ]增加了一个独立性约束，迫使学习到的去纠缠因子彼此不相似，我们不包含这样的正则化项，只监督学习到的LS -项兴趣的表示与它们相应的代理相似。这也是为什么我们不使用像InfoNCE [ 33 ]那样的损失函数，因为它对相反的编码器和代理之间的相似性施加了过强的惩罚。

  ​	总之，我们实现了两个独立的编码器φ和ψ，分别用于学习LS项兴趣的表示。为了实现LS -长期利益的解纠缠，我们从历史交互序列中计算代理。我们进一步提出了对比学习损失函数，以自监督的方式引导两个编码器仅捕获期望的方面。



*****

分析部分：

* Q：再仔细解读对比学习

  * 对比学习（Contrastive Learning）是一种在无监督学习或自监督学习中常用的技术，主要用于训练模型以区分不同的数据样本。对比学习的核心思想是通过将相似（正）样本的特征拉近，将不相似（负）样本的特征推远，来学习数据的有效表示

* Q：对比学习在本文

  * 对比学习被用于推荐系统中的长短期兴趣（LS-term interests）的解耦。这是为了更有效地捕捉和区分用户的长期兴趣和短期兴趣，从而提高推荐系统的准确性和效果。

* Q：在这里的自监督学习，自监督在哪

  * 通过对比学习，模型被训练以使得长期兴趣的表征更接近长期兴趣的代理（proxy），而远离短期兴趣的代理，反之亦然。这有助于模型更准确地区分和理解用户的长短期兴趣。

* Q：为什么对比学习做解藕能够辅助推荐系统做更好的预测

  * 通过更好地理解和区分用户的长短期兴趣，推荐系统可以更准确地预测用户可能感兴趣的物品，从而提高推荐的相关性和用户满意度。

* Q：长短期兴趣的前期处理

  * 论文中使用两种不同的编码器（注意力编码器）来分别生成长期兴趣和短期兴趣的表征。这些表征捕捉了用户在长时间跨度和短时间跨度内的行为模式。

* Q：BPR损失函数？

  * BPR（Bayesian Personalized Ranking）损失函数是一种在推荐系统领域广泛使用的优化准则，特别是在基于隐式反馈（如点击、浏览等）的个性化排名任务中。BPR旨在通过个性化的方式优化物品的排名顺序，使得用户更有可能喜欢的物品排名更高。

    BPR损失函数的核心思想是对于给定的用户，其实际交互过的物品（正样本）应该比未交互过的物品（负样本）有更高的预测评分。这种方法认为用户实际交互过的物品比未交互过的物品更能反映用户的偏好。

    BPR损失函数的数学表达式通常如下：

    $\mathcal{L}_{BPR} = - \sum_{(u, i, j) \in D_S} \log \sigma(\hat{r}_{ui} - \hat{r}_{uj}) $

    其中：
    - \( u \) 是用户。
    - \( i \) 是用户 \( u \) 交互过的物品（正样本）。
    - \( j \) 是用户 \( u \) 没有交互过的物品（负样本）。
    - \($D_S$ \) 是所有这样的用户-物品对的集合。
    - \($ \hat{r}_{ui} $\) 和 \( $\hat{r}_{uj}$ \) 分别是模型预测的用户 \( u \) 对物品 \( i \) 和 \( j \) 的评分。
    - \( $\sigma$ \) 是sigmoid函数，用于将评分差转化为概率。

    BPR损失函数的目标是最大化用户实际交互过的物品的预测评分与未交互过的物品的预测评分之间的差异。通过这种方式，模型学习产生一个个性化的物品排名，更好地满足用户的偏好。在实际应用中，通常还会在BPR损失函数中加入正则化项来防止过拟合。

* 对比学习的损失函数

  在您之前提到的对比学习的上下文中，用于长短期兴趣解耦的对比损失函数采用了两种形式：基于贝叶斯个性化排名 (BPR) 的损失函数和基于三元组 (Triplet) 的损失函数。这两种损失函数分别用于评估模型如何区分和理解长期和短期兴趣。下面是这两个损失函数的具体形式：

  1. **BPR损失函数**：
     \[ $\mathcal{L}_{\text{bpr}}(a, p, q) = \sigma(\langle a, q \rangle - \langle a, p \rangle) $\]
     其中：
     
     - \( a \) 是锚点样本（通常是用户的长或短期兴趣表征）。
     - \( p \) 是正样本（与锚点相对应的兴趣代理）。
     - \( q \) 是负样本（与锚点相反的兴趣代理）。
     - \( $\sigma$ \) 是softplus激活函数。
     - \( $\langle \cdot, \cdot \rangle$ \) 表示两个嵌入的内积。
     
     BPR损失函数的目的是使锚点样本与正样本的内积大于与负样本的内积，这样可以增强长短期兴趣的区分。
     
  2. **三元损失函数**：
     \[$ \mathcal{L}_{\text{tri}}(a, p, q) = \max \{d(a, p) - d(a, q) + m, 0\}$ \]
     其中：
     
     - \($ d(\cdot, \cdot)$ \) 表示两个嵌入之间的欧几里得距离。
     - \( $m$ \) 是一个正的边际值。
     
     三元损失函数的目的是使锚点样本与正样本之间的距离小于与负样本之间的距离加上一个边际值 \( m \)。

  这两个损失函数都用于强化模型对长短期兴趣代理的区分能力，即使模型能够更准确地识别和区分用户的长期和短期兴趣。通过这种方式，推荐系统可以更有效地根据用户的不同兴趣做出更准确的推荐。

* 再阅读这个公式

  当然可以，我会在公式符号旁边加上美元符号（$）以表示数学公式。下面是修改后的内容：

  这个公式 $\mathcal{L}_{\text{con}}^{u, t} = f(\boldsymbol{u}_l, \boldsymbol{p}_l, \boldsymbol{p}_s) + f(\boldsymbol{p}_l, \boldsymbol{u}_l, \boldsymbol{u}_s) + f(\boldsymbol{u}_s, \boldsymbol{p}_s, \boldsymbol{p}_l) + f(\boldsymbol{p}_s, \boldsymbol{u}_s, \boldsymbol{u}_l)$ 是对比学习中用于自监督解耦长短期兴趣的对比损失函数的表达式。具体地，它表示了四个对比任务的总和，每个任务都旨在通过一个函数 $f$ 来衡量不同样本之间的关系。这里的函数 $f$ 可以是基于贝叶斯个性化排名 (BPR) 的损失函数或三元损失函数。

  公式中的每一项代表一个特定的对比任务：
  1. $f(\boldsymbol{u}_l, \boldsymbol{p}_l, \boldsymbol{p}_s)$：这个任务确保用户的长期兴趣表征 $\boldsymbol{u}_l$ 更接近长期兴趣的代理 $\boldsymbol{p}_l$，而不是短期兴趣的代理 $\boldsymbol{p}_s$。
  2. $f(\boldsymbol{p}_l, \boldsymbol{u}_l, \boldsymbol{u}_s)$：这个任务确保长期兴趣的代理 $\boldsymbol{p}_l$ 更接近用户的长期兴趣表征 $\boldsymbol{u}_l$，而不是短期兴趣表征 $\boldsymbol{u}_s$。
  3. $f(\boldsymbol{u}_s, \boldsymbol{p}_s, \boldsymbol{p}_l)$：这个任务确保用户的短期兴趣表征 $\boldsymbol{u}_s$ 更接近短期兴趣的代理 $\boldsymbol{p}_s$，而不是长期兴趣的代理 $\boldsymbol{p}_l$。
  4. $f(\boldsymbol{p}_s, \boldsymbol{u}_s, \boldsymbol{u}_l)$：这个任务确保短期兴趣的代理 $\boldsymbol{p}_s$ 更接近用户的短期兴趣表征 $\boldsymbol{u}_s$，而不是长期兴趣表征 $\boldsymbol{u}_l$。

  总的来说，这个损失函数的目的是通过对比学习来加强长期和短期兴趣的区分，从而提高推荐系统在理解用户兴趣方面的准确性。通过这种方法，模型能更好地捕捉用户的兴趣变化，从而提供更准确的个性化推荐。

* 如何自己实现一个对比学习的代码过程

  ```python
  import torch
  import torch.nn as nn
  import torch.nn.functional as F
  
  class LongShortTermInterestModel(nn.Module):
      def __init__(self, num_users, num_items, embedding_dim, hidden_dim):
          super(LongShortTermInterestModel, self).__init__()
          self.user_embedding = nn.Embedding(num_users, embedding_dim)
          self.item_embedding = nn.Embedding(num_items, embedding_dim)
          self.long_term_encoder = nn.GRU(embedding_dim, hidden_dim, batch_first=True)
          self.short_term_encoder = nn.GRU(embedding_dim, hidden_dim, batch_first=True)
  
      def forward(self, user_ids, item_sequences):
          # 用户嵌入
          user_embed = self.user_embedding(user_ids)
          # 物品序列嵌入
          item_embed_seq = self.item_embedding(item_sequences)
          # 长期兴趣编码
          _, long_term_interest = self.long_term_encoder(item_embed_seq)
          # 短期兴趣编码
          _, short_term_interest = self.short_term_encoder(item_embed_seq)
          return long_term_interest, short_term_interest
  
  def bpr_loss(anchor, positive, negative):
      pos_score = torch.sum(anchor * positive, dim=-1)
      neg_score = torch.sum(anchor * negative, dim=-1)
      return -torch.mean(F.logsigmoid(pos_score - neg_score))
  
  def triplet_loss(anchor, positive, negative, margin=1.0):
      distance_positive = torch.sum((anchor - positive) ** 2, dim=-1)
      distance_negative = torch.sum((anchor - negative) ** 2, dim=-1)
      return torch.mean(F.relu(distance_positive - distance_negative + margin))
  
  
  ```

  





