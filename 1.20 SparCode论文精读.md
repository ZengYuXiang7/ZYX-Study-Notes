## 谨记：带着问题去做事情

- Q：SparCode论文是做什么的

  - 新的匹配范式Spar Code，它不仅支持复杂的特征交互，还支持高效的检索。
  - Spar Code支持复杂形式、性能优越的全对全交互模型，并使用稀疏倒排索引代替ANN框架。

- Q：什么是双塔模型

  - 双塔模型（Two-Tower Model）是一个在机器学习领域，尤其是推荐系统和信息检索中常用的深度学习架构。它被称为“双塔”是因为它通常包含两个主要的神经网络组件或“塔”，这两个塔分别学习表示用户和项目的特征
  - 在双塔模型中，一个塔用于处理和学习用户的行为或属性，称为用户塔，另一个塔用于处理和学习物品的特征，称为物品塔。这两个塔独立地学习表示，然后在模型的顶部通过某种形式的相似度计算（如点积或余弦相似度）来比较和组合这些表示。
  - 这种结构的优点是效率和灵活性。由于用户和物品的表示是分别学习的，所以它们可以独立地更新和检索，这使得模型可以很好地扩展到大型数据集。双塔模型在推荐系统中特别受欢迎，因为它可以有效地处理大量用户和物品的动态交互，并能灵活适应不同的信息源和特征类型。

- Q：双塔模型有何优缺点

  - 两个塔之间只有一个点积交互，导致对查询和项目之间细粒度特征交互的建模能力有限。虽然点积是一种很好的特征交互方法[ 27 ]，但它并不总是最优的，尤其是在内容特征丰富的情况下。
  - 在许多工业场景中，候选项目池的规模非常大，由于推理时间的限制，穷举比较所有候选项目变得不切实际。ANN检索方法( e.g . , HNSW )已经被用于构建索引和加快检索速度，但遗憾的是这可能会降低召回率，因为模型和索引通常不是端到端的训练。

- Q：点积和内积是不是一个意思

  - 

- Q：翻译成人话，到底有何优缺点

  - 点积不行，
  - 穷举有效率严重问题
    - 为了改进，构建索引，
    - 采用HNSW，构建索引的方式加快索引，但是他可能降低了召回率
    - 因为他不是端到端训练

- Q：什么事HNSW

  - 

- Q：就因为他不是端到端训练他就变捞了（文章给出答案）

  - 模型精度考虑（似乎没有很好回答端到端问题）

    - 仅仅只是交互建模限制？

  - ## 模型存储考虑

    - 用户数量太大，无法穷举，导致预计算代价太大。
    - 每个用户(索引)对应大量的物品，带来了极大的存储压力。

- Q：作者如何缓解

  - 精度问题：为了增强特征交互能力，全互换交互模块使用单个表达能力编码器捕获所有查询特征和所有项目特征之间的细粒度交互。
  - 存储问题：Spar Code利用矢量量化( VQ )作为全向交互和稀疏倒排索引之间的桥梁。查询被量化为一系列离散的代码和相应的代码表示，其中代码代替查询作为索引，代码表示用于所有到所有的交互。由于编码数量可控且远小于查询次数，缓解了索引数量大的问题。进一步地，Spar Code设计了一个受控的稀疏得分函数，使得每个指标只保存最相关的候选，大大缓解了存储压力。

- Q：精度问题是怎么解决的

  

  - 待定

- Q：矢量量化是什么东西

  - 矢量量化（Vector Quantization, VQ）是一种信号处理和模式识别领域中的基本技术，用于压缩和表示数据。
  - 在矢量量化中，整个数据空间被划分为有限数量的区域，每个区域被一个代表点或者“码字”（codeword）所代表，所有这些代表点的集合称为码书（codebook）。

- Q：已有的工作是怎么使用矢量量化的

  - 传统的Product Quantization采用后处理的方式将向量转化为多个离散编码，以加快最近邻搜索的速度。
  - 一般情况下，通过深度量化得到的代码表示作为查询表示的替代来表达相似的信息。

- Q：作者是如何使用矢量量化的

  - 使用VQ为倒排索引生成编码
  - 查询对应的代码表示
  - 进行全互换与项表示的交互

- Q：作者的模型分为几大模块，几个子模块

  - 大模块：全对全交互模型和矢量量化
  - 小模块：
    - Tokenizer，Quantizer，All-to-all Interaction-based Scorer，
    - Indexing and Retrieval

- 区别于双塔模型，有什么不同

  - 建模方式
    - Spar Code支持先进的全对全交互编码器，
    - 比仅支持点积等无参数交互的双塔匹配提供了更好的特征交互能力
    - Spar Code的量化器允许在不同的查询表示中共享子嵌入
    - 在双塔模型中每个查询的表示是独立的
  - 推理方式
  - Spar Code不缓存项目嵌入，
    - 而是选择性地缓存预先计算好的(代码,项)分值。
    - 
    - Spar Code cache是基于稀疏码的倒排索引的稀疏哈希表
    - 而双塔匹配缓存矩阵的项目嵌入或其他索引结构取决于ANN的设置

- Q：什么是全对全交互编码器

  - 一个参数化的、可学习的评分器，支持查询和项目之间的复杂交互，称为全互连基于评分的评分器。
  - 
  - $$
    S_i=M L P s\left(\left[s g\left[\widetilde{T}_i^u\right] \odot T_1^i ; \cdots ; s g\left[\widetilde{T}_i^u\right] \odot T_{K_c}^i\right]\right), i \in\{1,2, \cdots, K\}
    $$

  * 其中，$S_i \in \mathcal{R}^1$ 表示代码与候选项 $c$ 之间的匹配分数；$sg[\cdot]$ 表示停止梯度操作。

  * 由于码书和模型的其余部分是分别优化的，引入 $sg[\cdot]$ 是为了避免影响码书的参数。

  * 此外，查询由 $K_u$ 个令牌嵌入表示，上述评分函数将分别得到 $K_u$ 个分数。

  * $$
    \begin{aligned}
    &\hat{y}_i=\operatorname{ReLU}(S_i + \mathbf{b})\\
    &f(q, c)=\hat{y}=\sum_{i=1}^K\hat{y}_i
    \end{aligned}
    $$

- Q：码书是什么

  - 这个码书由一组离散的向量或代码构成，用于将连续的输入映射到离散的输出。
    - 在自编码器和生成模型中，码书通常用于将潜在空间中的连续表示离散化。这有助于减小模型的复杂性和存储需求。
    - 码书中的每个向量表示潜在的模型状态或特征，并且通过对输入数据进行量化，模型可以使用这些码书向量来表示输入。

- Q：码书是如何构建的

  - Tokenizer：

  - 将查询（用户）编码成多个 Token 嵌入，以表示用户的多个兴趣点。Token 嵌入是为了支持稀疏倒排索引而进行的编码。下面是对这段文字的简要解释：

    在推荐系统中，为了支持稀疏倒排索引，作者使用 Tokenizer 将查询 $q$ 编码成多个 Token 嵌入，以表示用户的多个兴趣点。这个 Tokenizer 的作用是将查询表示为 $K_u$ 个 tokens。查询可以包括图像、文本、序列信息或类别特征。无论查询是什么类型的数据，作者都将查询 $q \in Q$ 和候选项 $c \in \mathcal{I}$ 表示为一系列嵌入。

    具体而言，对于给定的查询 $q$ 和候选项 $c$，它们的嵌入分别表示为 $H^u$ 和 $H^v$，其中 $H^u$ 是查询 $q$ 的 $L$ 个嵌入，$H^v$ 是候选项 $c$ 的 $P$ 个嵌入，$D$ 是每个嵌入的维度。然后，作者将第 $i$ 个 token 的表示形式形式化如下：
    $$
    T_i^u=\operatorname{Tokenizer}\left(H^u\right) \in \mathbb{R}^{D^T},
    $$
    其中 $D^T$ 是 token 的嵌入维度，取决于 Tokenizer 的设置，通常等于 $D$。Tokenizer 的具体形式取决于给定的任务和特征，例如，如果查询是用户历史点击的序列，Tokenizer 可以选择使用 GRU [4]、Self-Attention [12, 17]、Capsule Network [17] 等。

    同样，作者将物品表示为 $K_c$ 个 tokens $T^c \in \mathbb{R}^{K_c \times D^T}$。这段文字主要介绍了在推荐系统中如何使用 Tokenizer 对查询和候选项进行编码，以便支持后续的模型操作。

* 举例这个过程

现在有两个嵌入向量 $H^u$ 和 $H^v$ 分别表示用户查询和候选商品。这两个向量可能包括文本描述、图像特征等信息。而 Tokenizer 的目标是将这些嵌入向量表示成一组 tokens，以便后续的推荐模型使用。这里的 Tokenizer 的作用是将每个嵌入向量编码为一组 token。

假设我们有一个用户查询向量 $H^u$，其包括文本嵌入和图像嵌入，以及一个商品向量 $H^v$，也包括文本嵌入和图像嵌入。这两个向量的维度为 $D$。 

1. **文本和图像嵌入向量：**
   - $H^u = [h_1^u, h_2^u, \ldots, h_L^u] \in \mathbb{R}^{L \times D}$ 表示用户查询向量，其中 $L$ 是查询中的元素数量。
   - $H^v = [h_1^v, h_2^v, \ldots, h_P^v] \in \mathbb{R}^{P \times D}$ 表示商品向量，其中 $P$ 是商品中的元素数量。

2. **Tokenizer 的应用：**
   - 对于用户查询向量 $H^u$，Tokenizer 将其应用于得到一组 tokens $T^u$，表示用户的多个兴趣点。这可能涉及将文本描述和图像特征结合起来。
   - 对于商品向量 $H^v$，Tokenizer 将其应用于得到一组 tokens $T^v$，表示商品的多个特征。

3. **Token 表示的使用：**
   - 现在，我们可以将 $T^u$ 和 $T^v$ 用于后续推荐模型的操作，比如计算它们之间的相似度、进行推荐等。

在这个例子中，Tokenizer 的具体形式可能取决于任务和特征的性质。例如，如果文本描述是序列信息，可以选择使用循环神经网络（RNN）作为 Tokenizer。如果图像是主要特征，可以使用卷积神经网络（CNN）或其他图像处理技术。选择 Tokenizer 的具体架构和参数通常取决于数据的特点和任务的需求。

* Q：量化器是什么

  先看原文

  3.2.2 量化器（Quantizer）

  ​	在文本匹配任务中，尽管查询的数量很大，它们共享相同的标记表（即词汇表），这限制了反向索引中的索引数量为标记表的大小。然而，查询标记嵌入是密集的，并且在不同的查询之间不共享，这使得构建合理数量的索引变得不可能。因此，在这一节中，量化器将查询标记嵌入转换为共享的代码及其表示，通过离散化实现。

  ​	在矢量量化 [29] 中，码书指的是一系列编号的向量，其编号称为代码。量化的过程是输入查询并通过查找码书返回代码及其对应的向量，从而使得任意查询共享同一码书。通过利用矢量量化，我们将标记嵌入量化为离散的代码。我们构建了 $M$ 个码书 $C \in \mathbb{R}^{N \times \frac{D^T}{M}}$，每个大小为 $N$，即包含 $N$ 个有序嵌入。对于 $T_i^u$，我们将其分割成 $M$ 个子嵌入 $T_i^{u,(m)}$ 并更新如下：
  $$
  \begin{aligned}
  \widetilde{T}_i^{u,(m)} & = \text{Quantizer}\left(T_i^{u,(m)}, C^{(m)}\right) \\
  & = C_k^{(m)}, \text{ 其中 } k = \arg \min_j \left\|C_j^{(m)} - T_i^{u,(m)}\right\|_2,
  \end{aligned}
  $$
  ​	其中 $C^{(m)}$ 是第 $m$ 个码书，$C_k^m$ 是其第 $k$ 个嵌入；Quantizer 表示从给定码书中查找最相似的子标记嵌入，相似性的定义取决于两个子嵌入之间的欧氏距离。因此，我们得到完整的更新后的标记嵌入：
  $$
  \widetilde{T}_i^u = \text{Concat}\left(\widetilde{T}_i^{u,(1)}, \widetilde{T}_i^{u,(2)}, \cdots, \widetilde{T}_i^{u,(M)}\right).
  $$
  ​	我们使用替换了的 $M$ 个子嵌入的索引来组合相应的离散代码。例如，在图中假设 $M=2$，并且第 2 个和第 $N$ 个子嵌入分别来自 $C^{(1)}$ 和 $C^{(2)}$，那么离散代码就是 $(2, N)$。

  ​	在码书的设计上有两方面的考虑。首先，每个码书的大小 $N$ 不应设置得太大，因为过大的码书会影响我们的 "Quantizer" 的速度。其次，$N$ 不应设置得太小，因为我们需要足够的模型容量来表示不同的查询。鉴于这些考虑，我们讨论了 $M$ 和 $N$ 的选择。离散代码的数量最多为 $N \times N \times \cdots \times N = N^M$。如果 $M$ 取 1，无论有多少查询，最终只会有 $N$ 种不同的查询嵌入。为了充分表示不同的查询，$N$ 倾向于设置得很大，但这会迅速增加参数的数量并可能减慢量化的速度。如果 $M$ 大于或等于 2，则可以使用较少的参数和更快的量化速度实现足够数量的查询。

  ​	此外，由于 "arg min" 是一个不可微分的操作，这里存在一个非常重要的优化问题。具体来说，原始标记嵌入 $T_i^u$ 无法从更新后的标记嵌入 $\widetilde{T}_i^u$ 中获得梯度，这最终导致之前的参数（例如，Tokenizer）不会被更新。我们将在第 3.2.4 节中描述相应的模型训练解决方案。

* Q：如何正确理解这个过程，举个例子？

  当嵌入向量到矢量量化的过程，可以用一个简单的例子来说明。假设我们有一个推荐系统，其中用户和商品的嵌入向量需要进行矢量量化。

  1. **初始嵌入向量：**
     - 用户向量 $H^u = [h_1^u, h_2^u, h_3^u]$，其中每个 $h_i^u$ 是用户的初始嵌入向量。
     - 商品向量 $H^v = [h_1^v, h_2^v, h_3^v]$，其中每个 $h_i^v$ 是商品的初始嵌入向量。

  2. **矢量量化过程：**
     - 使用矢量量化，我们构建了一个码书 $C = [c_1, c_2, c_3]$，其中每个 $c_i$ 是一个离散的代码向量。
     - 对于用户向量 $H^u$，我们通过查找最接近的码书向量，将每个初始嵌入向量量化为一个代码。
       - 例如，$\text{Quantizer}(h_1^u, C) = c_2$ 表示用户的第一个初始嵌入向量被量化为码书中的第二个代码。
       - 类似地，$\text{Quantizer}(h_2^u, C) = c_1$，$\text{Quantizer}(h_3^u, C) = c_3$。
     - 对于商品向量 $H^v$，也进行相同的矢量量化。

  3. **更新后的向量表示：**
     - 将量化后的代码组合成新的向量表示。
       - 对于用户向量 $H^u$，更新后的向量 $\widetilde{H}^u = [\text{Quantizer}(h_1^u, C), \text{Quantizer}(h_2^u, C), \text{Quantizer}(h_3^u, C)]$。
       - 对于商品向量 $H^v$，也有相应的更新后的向量 $\widetilde{H}^v$。

  在这个简单的案例中，我们通过矢量量化将连续的嵌入向量转换为了一组离散的代码向量。这个过程可以帮助减少模型的存储需求和提高推理速度，同时保留了原始信息的一部分。需要注意的是，具体的量化效果和码书的设计会影响最终的模型性能，因此需要根据具体任务进行调优。

* Q：所以码书是怎么构建的呢？

  在文中提到的方法中，码书是通过对嵌入向量进行矢量量化（Vector Quantization，VQ）的过程中学习得到的。具体地，论文中提到了构建 $M$ 个码书，每个码书的大小为 $N$。

  以下是构建码书的步骤：

  1. **初始化码书：** 在训练开始时，可以通过一些初始化方法（例如随机初始化）得到初始的码书。初始码书的大小为 $N$，其中 $N$ 是离散的代码数量。

  2. **矢量量化训练：** 在训练阶段，通过对嵌入向量进行矢量量化的方式来更新码书。这个过程是通过将嵌入向量映射到码书中最近的向量（根据欧氏距离或其他相似性度量）来实现的。

     对于每个嵌入向量 $T_i^u$ 中的子嵌入 $T_i^{u,(m)}$，通过以下步骤更新码书中的向量：

     $$
     \widetilde{T}_i^{u,(m)} = \text{Quantizer}(T_i^{u,(m)}, C^{(m)}) = C_k^{(m)}, \text{ 其中 } k = \arg \min_j \left\|C_j^{(m)} - T_i^{u,(m)}\right\|_2
     $$

     其中，$C^{(m)}$ 是第 $m$ 个码书，$C_k^{(m)}$ 是该码书中的第 $k$ 个向量。

  3. **更新码书：** 通过大量的训练样本，不断地通过矢量量化过程更新码书中的向量，使得码书能够更好地捕捉输入数据的特征。更新码书的方式可能涉及到梯度下降等优化方法，以减小量化误差。

  需要注意的是，初始码书的质量和更新码书的方式直接影响了矢量量化的性能。选择适当的初始化策略和优化方法是矢量量化算法中的关键步骤。

* Q：能不能用python简单实现逻辑过程

  当涉及到构建码书和进行矢量量化的过程时，可以使用 PyTorch 进行演示。以下是一个简化的示例，仅用于说明概念，并非真实的推荐系统应用：

  ```python
  import torch
  import torch.nn.functional as F
  
  # 初始化用户和商品的嵌入向量
  user_embeddings = torch.randn((3, 4), requires_grad=True)  # 3个用户，每个用户的嵌入向量维度为4
  item_embeddings = torch.randn((3, 4), requires_grad=True)  # 3个商品，每个商品的嵌入向量维度为4
  
  # 初始化码书
  codebook_size = 2
  user_codebook = torch.randn((codebook_size, 4), requires_grad=True)
  item_codebook = torch.randn((codebook_size, 4), requires_grad=True)
  
  # 矢量量化过程
  def quantizer(embeddings, codebook):
      # 扩展维度以便计算欧氏距离
      embeddings_expanded = embeddings.unsqueeze(1)
      # 计算欧氏距离
      distances = torch.norm(embeddings_expanded - codebook, dim=2)
      # 找到最近的码书向量的索引
      indices = torch.argmin(distances, dim=1)
      # 使用索引从码书中获取最近的向量
      quantized_vectors = torch.index_select(codebook, 0, indices)
      return quantized_vectors
  
  # 更新码书的方式（简单的梯度下降，实际中可能使用更复杂的方法）
  optimizer = torch.optim.SGD([user_codebook, item_codebook], lr=0.01)
  
  # 模拟训练过程
  for epoch in range(1000):
      # 用户向量矢量量化
      quantized_user = quantizer(user_embeddings, user_codebook)
      # 商品向量矢量量化
      quantized_item = quantizer(item_embeddings, item_codebook)
  
      # 计算损失，这里简化为欧氏距离
      loss = F.mse_loss(user_embeddings, quantized_user) + F.mse_loss(item_embeddings, quantized_item)
  
      # 反向传播和优化
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
  
      if (epoch + 1) % 100 == 0:
          print(f'Epoch {epoch + 1}/{100}, Loss: {loss.item()}')
  
  # 打印最终的用户和商品码书
  print('Final User Codebook:')
  print(user_codebook)
  print('\nFinal Item Codebook:')
  print(item_codebook)
  
  ```

  请注意，这只是一个简单的演示示例，实际的推荐系统中可能需要更复杂的模型和训练过程。此外，在实际应用中，矢量量化可能还涉及到学习率、训练集的选择等超参数的调整

* Q：完整的理解这个过程（头脑风暴

  * 首先初始化嵌入向量
  * 定义好超参数，固定码书数量，固定码书尺寸，训练码书，训练得分器
  * **在线服务（Online Serving）**:
    - 当用户发起查询时，系统首先通过分词器和量化器找到对应的代码。
    - 然后，系统从稀疏倒排索引中检索与这些代码相关的、得分最高的项目集。
    - 最后，这些项目被汇总并根据最终得分排序，最高得分的项目作为推荐结果返回给用户。

* Q：为什么减小模型，存储开销

  * **减小参数数量：** 在原始的连续嵌入中，每个元素都是一个可学习的参数，而在矢量量化后，通过将多个连续的元素映射到一个码书中的向量，可以显著减少模型中的参数数量。这对于存储模型参数和减小模型大小非常重要。
  * **离散化表示：** 码书中的向量是离散的，而不是原始连续嵌入的浮点数。离散表示可以使用更少的位数进行存储，从而减小模型在内存中的占用空间。这对于嵌入向量较大且需要大量存储空间的情况尤为重要。

* Q：模型是如何检索到

  * 在SparCode中，一个查询首先被转化为$K_u$个代码，每个代码通过所有到所有的交互评分器与每个项目$c$计算得到一个分数$s_c^{code}$。由于缓存所有（代码，项目）对的分数可能导致存储成本过高，SparCode采取了稀疏检索的方法，仅保留与每个代码最相关的几个项目的分数，使得使用稀疏倒排索引成为可能。

    在SparCode中，使用0作为阈值来决定是否在推理时存储分数。该系统在训练时使用一个可控制的偏置项来代替原有的偏置项，称为“稀疏控制”，表示为公式：
    $$
    \hat{y}_i=\operatorname{ReLU}\left(S_i+\tilde{\mathbf{b}}\right) .
    $$
    根据延迟要求和内存限制，可以调整公式中的$\widetilde{\mathbf{b}}$来确定索引的稀疏程度。

    SparCode的在线服务流程如图3(b)所示。右侧和蓝色部分展示了分数是如何被缓存的，即每个代码与候选项目的分数被稀疏地缓存。需要注意的是，这些分数是预先计算好的。

    在线服务时，首先通过分词器获取标记嵌入，然后通过代码本查找代码。接着，从缓存中加载对应的项目分数和过滤后的候选项目集。例如，在图3(a)中，如果代码是$(1,1)$和$(1,2)$，我们读取分数集$\left\{s_9^{(1,1)}, s_{24}^{(1,1)}, s_9^{(1,2)}, s_{13}^{(1,2)}, s_{25}^{(1,2)}\right\}$并得到合并的项目集$\left\{c_9, c_{24}, c_{13}, c_{25}\right\}$。最后，使用公式11来获取最终分数，并将得分最高的项目作为推荐结果。

* Q：为什么使用矢量量化和稀疏索引：

  - **效率提升**：通过将查询转换为较少的代码，可以减少必须进行评分计算的项目数，从而提高检索效率。

  - **减少存储需求**：仅存储与查询最相关的项目得分，降低了存储空间的需求。

* Q：所以矢量量化是怎么做的

  * ### 类比解释：

    想象一下，你有一个巨大的书店，里面有成千上万的书。每本书（在这个例子中代表一个商品）都有其独特的特征，比如主题、作者、风格等。这些特征可以被抽象成一个多维空间中的一个点，即一个嵌入向量。同样地，每个顾客（用户）也可以根据他们的兴趣和偏好被表示为这个空间中的一个点。

    #### 矢量量化的应用：

    1. **创建码本（Codebook）**:
       - 矢量量化的第一步是创建一个“码本”。你可以将码本想象成一张包含了书店所有区域的地图。这张地图被划分成许多小区域，每个区域代表着书店中一组具有相似特征的书籍。这些区域就是“代码”。
    2. **量化用户查询**:
       - 当一个顾客（用户）进入书店并提出查询时（比如：“我想要一本关于历史的科幻小说”），这个查询就像是在地图上指向了特定的区域（代码）。这个查询被转换成一个嵌入向量，然后这个向量被映射到最接近的代码上。这个过程就像是找到了最能代表顾客兴趣的书籍区域。
    3. **检索和推荐**:
       - 一旦我们知道了顾客的查询对应于地图上的哪个区域（代码），我们就可以快速检索这个区域中的书籍（商品）。由于这个区域包含了与顾客兴趣最相关的书籍，我们可以从中选择一些来推荐给顾客。

* Q：用代码展示过程（以简单内积为例子）：

  ```python
  from sklearn.preprocessing import normalize
  
  # Step 1: 初始化向量
  # 创建用户查询和商品的嵌入向量
  user_query_embeddings = np.random.rand(5, 10)  # 假设有5个用户查询，每个查询10维
  product_embeddings = np.random.rand(5, 10)    # 假设有5个商品，每个商品10维
  
  # Step 2: 构建码书
  # 使用k-means算法构建码书，每个码书大小为N
  N = 4  # 码书中的代码数量
  M = 2  # 将每个嵌入向量分割成M个子嵌入
  D_T = user_query_embeddings.shape[1]  # 嵌入向量的维度
  
  # 分割用户查询的嵌入向量
  split_user_query_embeddings = np.array(np.split(user_query_embeddings, M, axis=1))
  
  # 为每个子嵌入构建一个码书
  codebooks = [KMeans(n_clusters=N, random_state=0).fit(split_user_query_embeddings[m]) for m in range(M)]
  
  # Step 3: 训练评分器和码书
  # 在此示例中，我们不单独训练评分器，但已经通过k-means算法训练了码书
  
  # Step 4: 量化用户查询
  # 将每个用户查询的嵌入向量分割成M个子嵌入，并量化
  quantized_queries = []
  for user_query in user_query_embeddings:
      # 分割用户查询嵌入向量
      split_query = np.split(user_query, M)
  
      quantized_query_parts = []
      for m in range(M):
          # 对于每个子嵌入，找到码书中最相似的代码
          closest_code_idx = codebooks[m].predict(split_query[m].reshape(1, -1))[0]
          closest_code = codebooks[m].cluster_centers_[closest_code_idx]
  
          # 将找到的代码添加到量化查询的部分
          quantized_query_parts.append(closest_code)
  
      # 将所有量化的部分合并为一个完整的量化查询
      quantized_query = np.concatenate(quantized_query_parts)
      quantized_queries.append(quantized_query)
  
  # Step 5: 检索和推荐
  # 计算量化后的用户查询与每个商品的相似度得分
  scores = cosine_similarity(quantized_queries, product_embeddings)
  # 推荐得分最高的商品
  recommended_products = np.argmax(scores, axis=1)
  
  # 输出推荐的商品索引
  recommended_products
  
  ```

* Q：单独的矢量量化是怎么做的

  ```python
  # Step 1: 准备数据
  # 假设有10个用户或商品的嵌入向量，每个向量有10个维度
  embeddings = np.random.rand(10, 10)
  
  # Step 2: 应用聚类算法
  # 使用k-means算法，假设我们想要创建一个包含4个代码的码本
  n_clusters = 4
  kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(embeddings)
  
  # Step 3: 生成码本
  # 码本中的代码即为聚类的中心点
  codebook = kmeans.cluster_centers_
  
  # 输出码本中的代码
  codebook
  ```

* 现在以完整过程来实现这个代码，包括mlp，内积，实现原文

  ```python
  from sklearn.cluster import KMeans
  from sklearn.neural_network import MLPRegressor
  import numpy as np
  
  # Step 1: 初始化向量
  # 创建用户查询和商品的嵌入向量
  user_query_embeddings = np.random.rand(5, 10)  # 假设有5个用户查询，每个查询10维
  product_embeddings = np.random.rand(5, 10)    # 假设有5个商品，每个商品10维
  
  # Step 2: 构建码书
  N = 4  # 码书中的代码数量
  M = 2  # 将每个嵌入向量分割成M个子嵌入
  D_T = user_query_embeddings.shape[1]  # 嵌入向量的维度
  
  # 分割用户查询的嵌入向量
  split_user_query_embeddings = np.array(np.split(user_query_embeddings, M, axis=1))
  
  # 为每个子嵌入构建一个码书
  codebooks = [KMeans(n_clusters=N, random_state=0).fit(split_user_query_embeddings[m]) for m in range(M)]
  
  # Step 3: 量化用户查询
  quantized_queries = []
  for user_query in user_query_embeddings:
      split_query = np.split(user_query, M)
      quantized_query_parts = []
      for m in range(M):
          closest_code_idx = codebooks[m].predict(split_query[m].reshape(1, -1))[0]
          closest_code = codebooks[m].cluster_centers_[closest_code_idx]
          quantized_query_parts.append(closest_code)
      quantized_query = np.concatenate(quantized_query_parts)
      quantized_queries.append(quantized_query)
  
  # Step 4: 训练评分器 - 结合内积和MLP的评分器
  mlp_scorer = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', random_state=0)
  
  # 生成训练数据
  X_train = []
  y_train = []
  for quantized_query in quantized_queries:
      for product_embedding in product_embeddings:
          dot_product = np.dot(quantized_query, product_embedding)
          feature = np.concatenate([quantized_query, product_embedding, [dot_product]])
          X_train.append(feature)
          y_train.append(np.random.rand())
  
  # 训练MLP模型
  mlp_scorer.fit(X_train, y_train)
  
  # Step 5: 检索和推荐
  recommended_products = []
  for quantized_query in quantized_queries:
      scores = []
      for product_embedding in product_embeddings:
          dot_product = np.dot(quantized_query, product_embedding)
          score = mlp_scorer.predict(np.array([np.concatenate([quantized_query, product_embedding, [dot_product]])]))
          scores.append(score)
      recommended_products.append(np.argmax(scores))
  
  # 输出推荐的商品索引
  recommended_products
  
  
  ```

  









分析迁移过程：

* 分析构建索引复杂度
  * 
* 分析检索复杂度
  * 
* 分析存储开销
  * 