## 谨记：带着问题去做事情

## 12月27日

- Q：SparCode论文是做什么的

  - 新的匹配范式Spar Code，它不仅支持复杂的特征交互，还支持高效的检索。
  - Spar Code支持复杂形式、性能优越的全对全交互模型，并使用稀疏倒排索引代替ANN框架。

- Q：什么是双塔模型

  - 双塔模型（Two-Tower Model）是一个在机器学习领域，尤其是推荐系统和信息检索中常用的深度学习架构。它被称为“双塔”是因为它通常包含两个主要的神经网络组件或“塔”，这两个塔分别学习表示用户和项目的特征
  - 在双塔模型中，一个塔用于处理和学习用户的行为或属性，称为用户塔，另一个塔用于处理和学习物品的特征，称为物品塔。这两个塔独立地学习表示，然后在模型的顶部通过某种形式的相似度计算（如点积或余弦相似度）来比较和组合这些表示。
  - 这种结构的优点是效率和灵活性。由于用户和物品的表示是分别学习的，所以它们可以独立地更新和检索，这使得模型可以很好地扩展到大型数据集。双塔模型在推荐系统中特别受欢迎，因为它可以有效地处理大量用户和物品的动态交互，并能灵活适应不同的信息源和特征类型。

- Q：双塔模型有何优缺点

  - 两个塔之间只有一个点积交互，导致对查询和项目之间细粒度特征交互的建模能力有限。虽然点积是一种很好的特征交互方法[ 27 ]，但它并不总是最优的，尤其是在内容特征丰富的情况下。
  - 在许多工业场景中，候选项目池的规模非常大，由于推理时间的限制，穷举比较所有候选项目变得不切实际。ANN检索方法( e.g . , HNSW )已经被用于构建索引和加快检索速度，但遗憾的是这可能会降低召回率，因为模型和索引通常不是端到端的训练。

- Q：点积和内积是不是一个意思

  - 

- Q：翻译成人话，到底有何优缺点

  - 点积不行，
  - 穷举有效率严重问题
    - 为了改进，构建索引，
    - 采用HNSW，构建索引的方式加快索引，但是他可能降低了召回率
    - 因为他不是端到端训练

- Q：就因为他不是端到端训练他就变捞了（文章给出答案）

  - 模型精度考虑（似乎没有很好回答端到端问题）

    - 仅仅只是交互建模限制？

  - ## 模型存储考虑

    - 用户数量太大，无法穷举，导致预计算代价太大。
    - 每个用户(索引)对应大量的物品，带来了极大的存储压力。

- Q：作者如何缓解

  - 精度问题：为了增强特征交互能力，全互换交互模块使用单个表达能力编码器捕获所有查询特征和所有项目特征之间的细粒度交互。
  - 存储问题：Spar Code利用矢量量化( VQ )作为全向交互和稀疏倒排索引之间的桥梁。查询被量化为一系列离散的代码和相应的代码表示，其中代码代替查询作为索引，代码表示用于所有到所有的交互。由于编码数量可控且远小于查询次数，缓解了索引数量大的问题。进一步地，Spar Code设计了一个受控的稀疏得分函数，使得每个指标只保存最相关的候选，大大缓解了存储压力。

- Q：精度问题是怎么解决的

  

  - 待定

- Q：矢量量化是什么东西

  - 矢量量化（Vector Quantization, VQ）是一种信号处理和模式识别领域中的基本技术，用于压缩和表示数据。
  - 在矢量量化中，整个数据空间被划分为有限数量的区域，每个区域被一个代表点或者“码字”（codeword）所代表，所有这些代表点的集合称为码书（codebook）。

- Q：已有的工作是怎么使用矢量量化的

  - 传统的Product Quantization采用后处理的方式将向量转化为多个离散编码，以加快最近邻搜索的速度。
  - 一般情况下，通过深度量化得到的代码表示作为查询表示的替代来表达相似的信息。

- Q：作者是如何使用矢量量化的

  - 使用VQ为倒排索引生成编码
  - 查询对应的代码表示
  - 进行全互换与项表示的交互

- Q：作者的模型分为几大模块，几个子模块

  - 大模块：全对全交互模型和矢量量化
  - 小模块：
    - Tokenizer，Quantizer，All-to-all Interaction-based Scorer，
    - Indexing and Retrieval

- 区别于双塔模型，有什么不同

  - 建模方式
    - Spar Code支持先进的全对全交互编码器，
    - 比仅支持点积等无参数交互的双塔匹配提供了更好的特征交互能力
    - Spar Code的量化器允许在不同的查询表示中共享子嵌入
    - 在双塔模型中每个查询的表示是独立的
  - 推理方式
  - Spar Code不缓存项目嵌入，
    - 而是选择性地缓存预先计算好的(代码,项)分值。
    - 
    - Spar Code cache是基于稀疏码的倒排索引的稀疏哈希表
    - 而双塔匹配缓存矩阵的项目嵌入或其他索引结构取决于ANN的设置

- Q：什么是全对全交互编码器

  - 一个参数化的、可学习的评分器，支持查询和项目之间的复杂交互，称为全互连基于评分的评分器。
  - 
  - $$
    S_i=M L P s\left(\left[s g\left[\widetilde{T}_i^u\right] \odot T_1^i ; \cdots ; s g\left[\widetilde{T}_i^u\right] \odot T_{K_c}^i\right]\right), i \in\{1,2, \cdots, K\}
    $$

  * 其中，$S_i \in \mathcal{R}^1$ 表示代码与候选项 $c$ 之间的匹配分数；$sg[\cdot]$ 表示停止梯度操作。

  * 由于码书和模型的其余部分是分别优化的，引入 $sg[\cdot]$ 是为了避免影响码书的参数。

  * 此外，查询由 $K_u$ 个令牌嵌入表示，上述评分函数将分别得到 $K_u$ 个分数。

  * $$
    \begin{aligned}
    &\hat{y}_i=\operatorname{ReLU}(S_i + \mathbf{b})\\
    &f(q, c)=\hat{y}=\sum_{i=1}^K\hat{y}_i
    \end{aligned}
    $$

- Q：码书是什么

  - 这个码书由一组离散的向量或代码构成，用于将连续的输入映射到离散的输出。
    - 在自编码器和生成模型中，码书通常用于将潜在空间中的连续表示离散化。这有助于减小模型的复杂性和存储需求。
    - 码书中的每个向量表示潜在的模型状态或特征，并且通过对输入数据进行量化，模型可以使用这些码书向量来表示输入。

- Q：码书是如何构建的

  - Tokenizer：

  - 将查询（用户）编码成多个 Token 嵌入，以表示用户的多个兴趣点。Token 嵌入是为了支持稀疏倒排索引而进行的编码。下面是对这段文字的简要解释：

    在推荐系统中，为了支持稀疏倒排索引，作者使用 Tokenizer 将查询 $q$ 编码成多个 Token 嵌入，以表示用户的多个兴趣点。这个 Tokenizer 的作用是将查询表示为 $K_u$ 个 tokens。查询可以包括图像、文本、序列信息或类别特征。无论查询是什么类型的数据，作者都将查询 $q \in Q$ 和候选项 $c \in \mathcal{I}$ 表示为一系列嵌入。

    具体而言，对于给定的查询 $q$ 和候选项 $c$，它们的嵌入分别表示为 $H^u$ 和 $H^v$，其中 $H^u$ 是查询 $q$ 的 $L$ 个嵌入，$H^v$ 是候选项 $c$ 的 $P$ 个嵌入，$D$ 是每个嵌入的维度。然后，作者将第 $i$ 个 token 的表示形式形式化如下：
    $$
    T_i^u=\operatorname{Tokenizer}\left(H^u\right) \in \mathbb{R}^{D^T},
    $$
    其中 $D^T$ 是 token 的嵌入维度，取决于 Tokenizer 的设置，通常等于 $D$。Tokenizer 的具体形式取决于给定的任务和特征，例如，如果查询是用户历史点击的序列，Tokenizer 可以选择使用 GRU [4]、Self-Attention [12, 17]、Capsule Network [17] 等。

    同样，作者将物品表示为 $K_c$ 个 tokens $T^c \in \mathbb{R}^{K_c \times D^T}$。这段文字主要介绍了在推荐系统中如何使用 Tokenizer 对查询和候选项进行编码，以便支持后续的模型操作。



现在有两个嵌入向量 $H^u$ 和 $H^v$ 分别表示用户查询和候选商品。这两个向量可能包括文本描述、图像特征等信息。而 Tokenizer 的目标是将这些嵌入向量表示成一组 tokens，以便后续的推荐模型使用。这里的 Tokenizer 的作用是将每个嵌入向量编码为一组 token。

假设我们有一个用户查询向量 $H^u$，其包括文本嵌入和图像嵌入，以及一个商品向量 $H^v$，也包括文本嵌入和图像嵌入。这两个向量的维度为 $D$。 

1. **文本和图像嵌入向量：**
   - $H^u = [h_1^u, h_2^u, \ldots, h_L^u] \in \mathbb{R}^{L \times D}$ 表示用户查询向量，其中 $L$ 是查询中的元素数量。
   - $H^v = [h_1^v, h_2^v, \ldots, h_P^v] \in \mathbb{R}^{P \times D}$ 表示商品向量，其中 $P$ 是商品中的元素数量。

2. **Tokenizer 的应用：**
   - 对于用户查询向量 $H^u$，Tokenizer 将其应用于得到一组 tokens $T^u$，表示用户的多个兴趣点。这可能涉及将文本描述和图像特征结合起来。
   - 对于商品向量 $H^v$，Tokenizer 将其应用于得到一组 tokens $T^v$，表示商品的多个特征。

3. **Token 表示的使用：**
   - 现在，我们可以将 $T^u$ 和 $T^v$ 用于后续推荐模型的操作，比如计算它们之间的相似度、进行推荐等。

在这个例子中，Tokenizer 的具体形式可能取决于任务和特征的性质。例如，如果文本描述是序列信息，可以选择使用循环神经网络（RNN）作为 Tokenizer。如果图像是主要特征，可以使用卷积神经网络（CNN）或其他图像处理技术。选择 Tokenizer 的具体架构和参数通常取决于数据的特点和任务的需求。

* Q：量化器是什么

  先看原文

  3.2.2 量化器（Quantizer）

  ​	在文本匹配任务中，尽管查询的数量很大，它们共享相同的标记表（即词汇表），这限制了反向索引中的索引数量为标记表的大小。然而，查询标记嵌入是密集的，并且在不同的查询之间不共享，这使得构建合理数量的索引变得不可能。因此，在这一节中，量化器将查询标记嵌入转换为共享的代码及其表示，通过离散化实现。

  ​	在矢量量化 [29] 中，码书指的是一系列编号的向量，其编号称为代码。量化的过程是输入查询并通过查找码书返回代码及其对应的向量，从而使得任意查询共享同一码书。通过利用矢量量化，我们将标记嵌入量化为离散的代码。我们构建了 $M$ 个码书 $C \in \mathbb{R}^{N \times \frac{D^T}{M}}$，每个大小为 $N$，即包含 $N$ 个有序嵌入。对于 $T_i^u$，我们将其分割成 $M$ 个子嵌入 $T_i^{u,(m)}$ 并更新如下：
  $$
  \begin{aligned}
  \widetilde{T}_i^{u,(m)} & = \text{Quantizer}\left(T_i^{u,(m)}, C^{(m)}\right) \\
  & = C_k^{(m)}, \text{ 其中 } k = \arg \min_j \left\|C_j^{(m)} - T_i^{u,(m)}\right\|_2,
  \end{aligned}
  $$
  ​	其中 $C^{(m)}$ 是第 $m$ 个码书，$C_k^m$ 是其第 $k$ 个嵌入；Quantizer 表示从给定码书中查找最相似的子标记嵌入，相似性的定义取决于两个子嵌入之间的欧氏距离。因此，我们得到完整的更新后的标记嵌入：
  $$
  \widetilde{T}_i^u = \text{Concat}\left(\widetilde{T}_i^{u,(1)}, \widetilde{T}_i^{u,(2)}, \cdots, \widetilde{T}_i^{u,(M)}\right).
  $$
  ​	我们使用替换了的 $M$ 个子嵌入的索引来组合相应的离散代码。例如，在图中假设 $M=2$，并且第 2 个和第 $N$ 个子嵌入分别来自 $C^{(1)}$ 和 $C^{(2)}$，那么离散代码就是 $(2, N)$。

  ​	在码书的设计上有两方面的考虑。首先，每个码书的大小 $N$ 不应设置得太大，因为过大的码书会影响我们的 "Quantizer" 的速度。其次，$N$ 不应设置得太小，因为我们需要足够的模型容量来表示不同的查询。鉴于这些考虑，我们讨论了 $M$ 和 $N$ 的选择。离散代码的数量最多为 $N \times N \times \cdots \times N = N^M$。如果 $M$ 取 1，无论有多少查询，最终只会有 $N$ 种不同的查询嵌入。为了充分表示不同的查询，$N$ 倾向于设置得很大，但这会迅速增加参数的数量并可能减慢量化的速度。如果 $M$ 大于或等于 2，则可以使用较少的参数和更快的量化速度实现足够数量的查询。

  ​	此外，由于 "arg min" 是一个不可微分的操作，这里存在一个非常重要的优化问题。具体来说，原始标记嵌入 $T_i^u$ 无法从更新后的标记嵌入 $\widetilde{T}_i^u$ 中获得梯度，这最终导致之前的参数（例如，Tokenizer）不会被更新。我们将在第 3.2.4 节中描述相应的模型训练解决方案。

* Q：如何正确理解这个过程，举个例子？

  当嵌入向量到矢量量化的过程，可以用一个简单的例子来说明。假设我们有一个推荐系统，其中用户和商品的嵌入向量需要进行矢量量化。

  1. **初始嵌入向量：**
     - 用户向量 $H^u = [h_1^u, h_2^u, h_3^u]$，其中每个 $h_i^u$ 是用户的初始嵌入向量。
     - 商品向量 $H^v = [h_1^v, h_2^v, h_3^v]$，其中每个 $h_i^v$ 是商品的初始嵌入向量。

  2. **矢量量化过程：**
     - 使用矢量量化，我们构建了一个码书 $C = [c_1, c_2, c_3]$，其中每个 $c_i$ 是一个离散的代码向量。
     - 对于用户向量 $H^u$，我们通过查找最接近的码书向量，将每个初始嵌入向量量化为一个代码。
       - 例如，$\text{Quantizer}(h_1^u, C) = c_2$ 表示用户的第一个初始嵌入向量被量化为码书中的第二个代码。
       - 类似地，$\text{Quantizer}(h_2^u, C) = c_1$，$\text{Quantizer}(h_3^u, C) = c_3$。
     - 对于商品向量 $H^v$，也进行相同的矢量量化。

  3. **更新后的向量表示：**
     - 将量化后的代码组合成新的向量表示。
       - 对于用户向量 $H^u$，更新后的向量 $\widetilde{H}^u = [\text{Quantizer}(h_1^u, C), \text{Quantizer}(h_2^u, C), \text{Quantizer}(h_3^u, C)]$。
       - 对于商品向量 $H^v$，也有相应的更新后的向量 $\widetilde{H}^v$。

  在这个简单的案例中，我们通过矢量量化将连续的嵌入向量转换为了一组离散的代码向量。这个过程可以帮助减少模型的存储需求和提高推理速度，同时保留了原始信息的一部分。需要注意的是，具体的量化效果和码书的设计会影响最终的模型性能，因此需要根据具体任务进行调优。

* Q：所以码书是怎么构建的呢？

  在文中提到的方法中，码书是通过对嵌入向量进行矢量量化（Vector Quantization，VQ）的过程中学习得到的。具体地，论文中提到了构建 $M$ 个码书，每个码书的大小为 $N$。

  以下是构建码书的步骤：

  1. **初始化码书：** 在训练开始时，可以通过一些初始化方法（例如随机初始化）得到初始的码书。初始码书的大小为 $N$，其中 $N$ 是离散的代码数量。

  2. **矢量量化训练：** 在训练阶段，通过对嵌入向量进行矢量量化的方式来更新码书。这个过程是通过将嵌入向量映射到码书中最近的向量（根据欧氏距离或其他相似性度量）来实现的。

     对于每个嵌入向量 $T_i^u$ 中的子嵌入 $T_i^{u,(m)}$，通过以下步骤更新码书中的向量：

     $$
     \widetilde{T}_i^{u,(m)} = \text{Quantizer}(T_i^{u,(m)}, C^{(m)}) = C_k^{(m)}, \text{ 其中 } k = \arg \min_j \left\|C_j^{(m)} - T_i^{u,(m)}\right\|_2
     $$

     其中，$C^{(m)}$ 是第 $m$ 个码书，$C_k^{(m)}$ 是该码书中的第 $k$ 个向量。

  3. **更新码书：** 通过大量的训练样本，不断地通过矢量量化过程更新码书中的向量，使得码书能够更好地捕捉输入数据的特征。更新码书的方式可能涉及到梯度下降等优化方法，以减小量化误差。

  需要注意的是，初始码书的质量和更新码书的方式直接影响了矢量量化的性能。选择适当的初始化策略和优化方法是矢量量化算法中的关键步骤。

* Q：能不能用python简单实现逻辑过程

  当涉及到构建码书和进行矢量量化的过程时，可以使用 PyTorch 进行演示。以下是一个简化的示例，仅用于说明概念，并非真实的推荐系统应用：

  ```python
  import torch
  import torch.nn.functional as F
  
  # 初始化用户和商品的嵌入向量
  user_embeddings = torch.randn((3, 4), requires_grad=True)  # 3个用户，每个用户的嵌入向量维度为4
  item_embeddings = torch.randn((3, 4), requires_grad=True)  # 3个商品，每个商品的嵌入向量维度为4
  
  # 初始化码书
  codebook_size = 2
  user_codebook = torch.randn((codebook_size, 4), requires_grad=True)
  item_codebook = torch.randn((codebook_size, 4), requires_grad=True)
  
  # 矢量量化过程
  def quantizer(embeddings, codebook):
      # 扩展维度以便计算欧氏距离
      embeddings_expanded = embeddings.unsqueeze(1)
      # 计算欧氏距离
      distances = torch.norm(embeddings_expanded - codebook, dim=2)
      # 找到最近的码书向量的索引
      indices = torch.argmin(distances, dim=1)
      # 使用索引从码书中获取最近的向量
      quantized_vectors = torch.index_select(codebook, 0, indices)
      return quantized_vectors
  
  # 更新码书的方式（简单的梯度下降，实际中可能使用更复杂的方法）
  optimizer = torch.optim.SGD([user_codebook, item_codebook], lr=0.01)
  
  # 模拟训练过程
  for epoch in range(1000):
      # 用户向量矢量量化
      quantized_user = quantizer(user_embeddings, user_codebook)
      # 商品向量矢量量化
      quantized_item = quantizer(item_embeddings, item_codebook)
  
      # 计算损失，这里简化为欧氏距离
      loss = F.mse_loss(user_embeddings, quantized_user) + F.mse_loss(item_embeddings, quantized_item)
  
      # 反向传播和优化
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
  
      if (epoch + 1) % 100 == 0:
          print(f'Epoch {epoch + 1}/{100}, Loss: {loss.item()}')
  
  # 打印最终的用户和商品码书
  print('Final User Codebook:')
  print(user_codebook)
  print('\nFinal Item Codebook:')
  print(item_codebook)
  
  ```

  请注意，这只是一个简单的演示示例，实际的推荐系统中可能需要更复杂的模型和训练过程。此外，在实际应用中，矢量量化可能还涉及到学习率、训练集的选择等超参数的调整

* Q：

* 