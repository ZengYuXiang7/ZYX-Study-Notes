# 3.29 MMSSL  自监督+多模态   自监督到底是什么

### Multi-Modal Self-Supervised Learning for Recommendation





这篇论文我本质上想解决什么问题，我对论文的一些执行思想，我需要深入理解。

1. 为了捕获协同关系和上下文特征的依赖关系，

2. 通过对抗网络，协同视图和多模态上下文视图之间的隐含相互依赖性建模可以通过在表示中实现对抗鲁棒性来增强。

3. 进一步提高对抗式SSL的鲁棒性，

4. 通过调整A，B和C的分布来捕获协同视图和多模态视图之间分布的相互依赖关系。

5. 捕捉每对模态特定xx之间的相关性，涉及了多头注意力机制。

6. 在xx情况下探索xx高阶协同性。

7. 最大化xx和xx的互信息

8. 聚合来促进对应某某xx和xx之间的合作。
9. 自监督信号是什么。





* ## 先看原论文

问题定义：

交互图与多模态。在图学习的背景下，基于图神经网络的协同过滤范式已经取得了最先进的结果。受此启发，MMSSL 是建立在图结构的交互数据上的。具体来说，我们生成一个用户-物品图 $G=\{(u, i) \mid u \in \mathcal{U}, i \in \mathcal{I}\}$，其中 $\mathcal{U}$ 和 $\mathcal{I}$ 分别表示用户和物品的集合。如果观察到了一次交互，则在 $G$ 中在用户 $u$ 和物品 $i$ 之间构建一条边。我们将多模态信息（例如，文本、视觉、声音模态）与生成的用户-物品交互图 $G$ 结合起来，其中物品 $i$ 具有模态感知特征 $\overline{\mathbf{F}}=\left\{\overline{\mathbf{f}}_i^1, \ldots, \overline{\mathbf{f}}_i^m, \ldots, \overline{\mathbf{f}}_i^{|\mathcal{M}|}\right\}$。这里，$\overline{\mathbf{f}}_i^m \in \mathbb{R}^{d_m}$ 表示具有模态 $m \in \mathcal{M}$ 的物品 $i$ 的原始特征嵌入（维度为 $d_m$）。

任务表述。我们构建了一个多模态推荐系统，通过模态感知的用户偏好学习来捕捉用户-物品关系。特别是，给定生成的多模态交互图 $G=\{(u, i) \mid u \in \mathcal{U}, i \in \mathcal{I}, \overline{\mathbf{F}}\}$​，我们的多模态推荐任务是学习一个函数，预测一个物品被一个用户采纳的可能性有多大。





在我们的模型中，我们设计了一个自我监督学习任务，通过多模式对抗用户-物品关系学习来补充用户-物品交互建模。得益于这种设计，我们的 MMSSL 不仅能捕捉用户对物品的模式化偏好，还能通过多模式上下文中的自监督信号有效缓解数据稀缺的问题。为了实现我们的目标，我们提出了一种对抗式自增强方法，该方法由模式引导的协作关系生成器 $\mathcal{G}(\cdot)$ 和图结构判别器 $\mathcal{D}(\cdot)$ 组成。



3.1.1 模态引导的协同关系生成器。在我们的自我增强方案的生成阶段中，我们旨在对用户-项目交互进行模态感知的图结构学习，以挖掘模态特定的用户偏好。

​	换句话说，我们旨在根据相应的多模态上下文推导用户 $u$ 与项目 $i$ 交互的可能性：
$$
\operatorname{Pro}\left(\hat{\mathbf{A}}^m \mid \mathbf{F}^m\right)=\operatorname{Pro}\left(\hat{\mathbf{A}}^m[u, i]=1 \mid \mathbf{f}_u^m, \mathbf{f}_i^m\right)
$$
其中 $\hat{A}^m \in \mathbb{R}^{|\mathcal{U}| \times|\mathcal{I}|}$ 表示在模态 $m$ 下学习的用户-项目交互矩阵。$\hat{A}^m$ 中的每个元素表示为 $\hat{\mathbf{A}}^m[u, i] \in \mathbb{R}$。

​	为了生成输入模态感知的用户和项目表示 $\mathbf{f}_u^m$ 和 $\mathbf{f}_i^m$，我们将协同效应结合到基于原始多模态特征向量 $\overline{\mathbf{f}}_i^m$ 的嵌入中：
$$
\mathbf{f}_u^m=\sum_{i \in \mathcal{N}_u} \overline{\mathbf{f}}_i^m / \sqrt{\left|\mathcal{N}_u\right|} ; \quad \mathbf{f}_i^m=\sum_{u \in \mathcal{N}_i} \mathbf{f}_u^m / \sqrt{\left|\mathcal{N}_i\right|}
$$

这里，$\mathcal{N}_u$ 和 $\mathcal{N}_i$ 分别表示用户 $u \in \mathcal{U}$ 和项目 $i \in \mathcal{I}$ 在用户-项目交互图 $G$ 中的邻域集。在将多模态特征向量输入到我们的生成器之前，我们使用具有 dropout [12] 的全连接层来执行模态特定的转换，将原始多模态特征映射到潜在嵌入空间，即 $\overline{\mathbf{f}}_i^m \in \mathbb{R}^{d_m} \longrightarrow \overline{\mathbf{f}}_i^m \in \mathbb{R}^d, d \ll d_m$。

为了捕获协同关系与多模态上下文特征之间的相互依赖关系，通过学习具有多模态表示的偏好矩阵 $\hat{\mathbf{A}}^m \in \mathbb{R}^{|\mathcal{U}| \times|\mathcal{I}|}$ 来进行模态感知的图结构学习：
$$
\hat{\mathbf{A}}^m[u, i]=\mathcal{G}\left(\mathbf{f}_u^m, \mathbf{f}_i^m\right)=\frac{\mathbf{f}_u^{m \top} \cdot \mathbf{f}_i^m}{\left\|\mathbf{f}_u^m\right\|_2 \cdot\left\|\mathbf{f}_i^m\right\|_2}
$$
3.1.2 具有对抗生成的鉴别器。

在我们的对抗式 SSL 范式中，鉴别器 $\mathcal{D}(\cdot)$ 被设计为区分生成的模态感知用户-项目关系 $\hat{\mathbf{A}}^m$ 和来自图 $G$ 的观察到的用户-项目交互矩阵 $\mathbf{A}$。通过我们的关系判别过程，我们的生成器试图通过优化学习到的模态感知关系矩阵来混淆鉴别器。通过这样做，协同视图和多模态上下文视图之间的隐含相互依赖性建模可以通过在表示中实现对抗鲁棒性来增强。形式上，鉴别器 $\mathcal{D}(\cdot)$ 构建如下：
$$
\mathcal{D}(\mathbf{a})=\delta\left(\Gamma^2(\mathbf{a})\right) ; \Gamma(\mathbf{a})=\operatorname{Drop}(\operatorname{BN}(\operatorname{LeakyRelu}(\operatorname{Linear}(\mathbf{a}))))
$$

按照文献 $[25,29]$ 中的学习策略，

输入嵌入 $\mathbf{a} \in \mathbb{R}^{|\mathcal{I}|}$ 对应于每个矩阵行，它们分别来自于生成的关系 $\hat{\mathbf{A}}^m$ 或观察到的交互 $\mathbf{A}$，即 $\left\{\mathbf{a} \mid \mathbf{a} \in \mathbf{A}\right.$ 或 $\left.\mathbf{a} \in \hat{\mathbf{A}}^m, m \in \mathcal{M}\right\} $.

$\Gamma(\cdot)$ 表示通过以下方式利用的鉴别神经层：i) 批次归一化 $\mathrm{BN}(\cdot)$ 以防止梯度消失问题 [15]；ii) dropout $\operatorname{Drop}(\cdot)$ 以减轻过拟合 [12]；iii) 非线性激活 LeakyRelu $(\cdot)$ 以促进模型收敛，获得更完整的梯度 $[13,23]$。我们在鉴别器中堆叠了两个全连接层，并采用 sigmoid 函数 $\delta(\cdot)$ 来逼近二进制交互数据的分布。Linear $(\cdot)$ 对 $\mathbf{a}$ 应用线性变换。



3.1.3 针对分布差异的对抗式 SSL。

与视觉数据的密集像素矩阵不同，观察到的用户-项目交互矩阵由于包含大量零值而高度稀疏。这种稀疏性为我们的对抗式模态感知关系学习带来了独特的挑战，原因在于生成器 $\mathcal{G}(\cdot)$ 和鉴别器 $\mathcal{D}(\cdot)$ 之间的数据分布差异。特别是，由生成器学习得到的具有密集值的用户-项目关系矩阵 $\hat{A}^m$ 与观察到的交互矩阵 $\mathbf{A}$ 不可避免地存在很大差异，这可能导致模式崩溃和收敛困难 [31]。

为了解决这一挑战，我们利用 Gumbel-Softmax [16] 将原始交互数据转换为基于 Gumbel 分布的密集矩阵，并弥合分布差距。具体而言，对抗性增强如下所示：
$$
\tilde{\mathbf{A}}[u, i]=\underbrace{\frac{\exp ((\mathbf{A}[u, i]+g) / \tau)}{\sum_{i^{\prime}} \exp \left(\left(\mathbf{A}\left[u, i^{\prime}\right]+g\right) / \tau\right)}}_{\text {transformation }}+\underbrace{\zeta \times \frac{\mathbf{h}_u^{\top} \mathbf{h}_i}{\left\|\mathbf{h}_u\right\|_2\left\|\mathbf{h}_i\right\|_2}}_{\text {augmented signals }}
$$

这里，扰动因子 $g$ 计算为 $g=-\log (-\log (u))$，基于 Gumbel 分布 $(0,1)$，其中 $u \sim \operatorname{Uniform}(0,1)$ [16]。通过基于 Gumbel 的转换，原始观察到的交互矩阵 $\mathbf{A}$ 可以投影到一个密集分布的矩阵 $\tilde{\mathbf{A}} \in \mathbb{R}^{|\mathcal{U}| \times|\mathcal{I}|}$。$\tau$ 是用于调整平滑度的温度因子。为了进一步提高对抗式 SSL 的鲁棒性，我们将最终的多模态协同嵌入 $\mathbf{h}_u, \mathbf{h}_i$ 注入辅助信号。$\zeta$​ 是一个权重参数。



3.1.4 对抗式 SSL 损失。

在我们的对抗式 SSL 任务中，我们旨在通过调整我们学习到的模态感知用户-项目关系 $\hat{\mathbf{A}}^m$ 和观察到的用户-项目交互 $\mathbf{A}$ 的代理 $\tilde{\mathbf{A}}$ 之间的分布来捕获协同视图和多模态视图之间的相互依赖关系。为此，我们定义了对抗式 SSL 损失，以最小最大化的方式优化我们的关系生成器 $\mathcal{G}(\cdot)$ 和鉴别器 $\mathcal{D}(\cdot)$ 如下：
$$
\min _{\mathcal{G}} \max _{\mathcal{D}} \mathbb{E}_{\tilde{\mathbf{A}} \sim \mathbb{P}_r}[\mathcal{D}(\tilde{\mathbf{A}})]-\mathbb{E}_{\hat{\mathbf{A}}^m \sim \mathbb{P}_f}\left[\left(\mathcal{D}\left(\mathcal{G}\left(\mathbf{f}^m\right)\right)\right]\right.
$$
我们分别提出了与生成器 $\mathcal{G}$ 和鉴别器 $\mathcal{D}$ 相对应的优化目标 $\mathcal{L}_{\mathrm{G}}$ 和 $\mathcal{L}_{\mathrm{D}}$。

$$
\left\{\begin{array}{l}
\mathcal{L}_{\mathcal{G}}=-\mathbb{E}_{\hat{\mathbf{A}}^m}\left[\mathcal{D}\left(\hat{\mathbf{A}}^m\right)\right] \\
\mathcal{L}_{\mathcal{D}}=\mathbb{E}_{\tilde{\mathbf{A}}}[\mathcal{D}(\tilde{\mathbf{A}})]-\mathbb{E}_{\hat{\mathbf{A}}^m}\left[\mathcal{D}\left(\hat{\mathbf{A}}^m\right)\right]+\lambda_1 \mathbb{E}_{\ddot{\mathbf{A}}}\left[\left(\left\|\nabla_{\mathcal{D}(\ddot{\mathbf{A}})}\right\|-1\right)^2\right]
\end{array}\right.
$$

为了进一步增强我们的对抗式自监督学习对分布差距和数据稀疏性的鲁棒性，我们引入了 WassersteinGANGP [9] 来添加梯度惩罚，并使用平衡权重 $\lambda_1$。这里，$\ddot{\mathbf{A}}$ 表示 $\hat{\mathbf{A}}^m$ 和 $\tilde{\mathbf{A}}$​ 矩阵的插值。

3.2 跨模态对比学习

在多媒体推荐场景中，用户与不同项目模态（例如，视觉、文本和声音）的交互行为模式会相互影响。例如，短视频的视觉和声音特征可以共同吸引用户观看。因此，用户的视觉特定和声音特定偏好可能以复杂的方式交织在一起。为了捕获用户模态特定偏好之间的这种隐含依赖关系，我们设计了一种具有模态感知图对比增强的跨模态对比学习范式。

3.2.1 模态感知对比视图。为了将模态特定语义注入到我们的对比学习组件中，我们对用户 $u$ 和项目 $i$ 的模态感知语义邻居 $\mathcal{N}_u^m$ 和 $\mathcal{N}_i^m$ 进行信息聚合。它可以从我们生成器中学习到的关系矩阵 $\hat{\mathbf{A}}^m$（式3）中推导出来。
$$
\mathbf{e}_u^m=\sum_{i \in \mathcal{N}_u^m} \mathbf{e}_i / \sqrt{\left|\mathcal{N}_u^m\right|} ; \quad \mathbf{e}_i^m=\sum_{u \in \mathcal{N}_i^m} \mathbf{e}_u / \sqrt{\left|\mathcal{N}_i^m\right|}
$$
其中，$\mathbf{e}_u, \mathbf{e}_i \in \mathbb{R}^d$ 是 Xavier 初始化的对应嵌入。多模态上下文信息可以在模态感知潜在表示 $\mathbf{e}_u^m, \mathbf{e}_i^m \in \mathbb{R}^d$​ 中得到保留。



模态间依赖建模。为了捕获每对模态特定用户偏好之间的相关性，我们设计了一个具有多头自注意机制的模态间依赖编码器，使用以下公式：
$$
\overline{\mathbf{e}}_u^m=\sum_{m^{\prime} \in \mathcal{M}}^{|\mathcal{M}|} \sum_{h=1}^H \sigma\left(\frac{\left(\mathbf{e}_u^m \mathbf{W}_h^Q\right)^{\top} \cdot\left(\tilde{\mathbf{e}}_u^{m^{\prime}} \mathbf{W}_h^K\right)}{\sqrt{d / H}}\right) \cdot \mathbf{e}_u^{m^{\prime}}
$$
其中，$\mathbf{W}_h^Q, \mathbf{W}_h^K \in \mathbb{R}^{d / H \times d}$ 表示用于计算模态对 $\left(m, m^{\prime}\right)$ 之间关系的 $h$-head 查询和键变换。$H$ 表示注意力头的数量。$\sigma(\cdot)$ 表示 softmax 函数。用户和项目方的嵌入以类似的方式进行了改进。然后，我们通过均值池化（例如 $\overline{\mathbf{e}}_u=\sum_{m \in \mathcal{M}}^{|\mathcal{M}|} \overline{\mathbf{e}}_u^m /|\mathcal{M}|$）融合了模态特定的嵌入，生成了多模态用户/项目表示 $\overline{\mathbf{e}}_u, \overline{\mathbf{e}}_i \in \mathbb{R}^d$​。



多模态高阶连接。为了在考虑多模态信息的情况下探索高阶协同效应，我们基于图神经网络构建了我们的编码器，使用以下矩阵形式进行递归消息传递：
$$
\hat{\mathbf{E}}_u^{l+1}=\mathbf{A} \cdot \hat{\mathbf{E}}_u^l ; \quad \hat{\mathbf{E}}_u^0=\mathbf{E}_u+\eta \cdot \overline{\mathbf{E}}_u /\left\|\overline{\mathbf{E}}_u\right\|_2^2
$$
其中，$\hat{\mathbf{e}}_u^l \in \hat{\mathbf{E}}_u^l, \hat{\mathbf{e}}_u^{l+1} \in \hat{\mathbf{E}}_u^{l+1}$ 分别表示第 $l$ 层和第 $(l+1)$ 层的嵌入。如果用户 $u$ 与项目 $i$ 进行了交互，则节点度归一化矩阵元素 $\mathbf{A}[u, i]=1 / \sqrt{\left|\mathcal{N}_u\right|}$。零阶嵌入 $\hat{\mathbf{E}}_u^0$ 是通过使用权重参数 $\eta$ 将初始 id 对应的嵌入 $\mathbf{E}_u$ 与归一化的模态感知嵌入 $\overline{\mathbf{E}}_u$ 结合而获得的。在我们的多层 GNN 中，通过均值池化聚合特定层的嵌入以得到输出：$\hat{\mathbf{E}}_u=\sum_{l=0}^L \hat{\mathbf{E}}_u^l / L$，其中 $L$​ 是图层的数量。



3.2.2 跨模态对比增强。在这个模块中，我们介绍了我们的多模态对比学习范式，它



如何提取了模态特定用户偏好之间依赖建模的自我监督信号。具体而言，我们采用 InfoNCE 损失函数来最大化模态特定嵌入 $\mathbf{e}_u^m$ 和相同用户 $u$ 的整体用户嵌入 $\mathbf{h}_u$ 之间的互信息。采用自我区分策略，来自不同用户的嵌入被视为负对。我们的跨模态对比损失定义如下：
$$
\begin{aligned}
\mathcal{L}_{\mathrm{CL}}= & \sum_{m \in \mathcal{M}}^{|\mathcal{M}|} \sum_{u \in \mathcal{U}}^{|\mathcal{U}|} \log \frac{\exp s\left(\mathbf{h}_u, \mathbf{e}_u^m\right)}{\sum_{u^{\prime} \in \mathcal{U}}\left(\exp s\left(\mathbf{h}_{u^{\prime}}, \mathbf{e}_u^m\right)+\exp s\left(\mathbf{e}_{u^{\prime}}^m, \mathbf{e}_u^m\right)\right)} \\
& s\left(\mathbf{h}_u, \mathbf{e}_u^m\right)=\mathbf{h}_u^{\top} \cdot \mathbf{e}_u^m /\left(\tau^{\prime} \cdot\left\|\mathbf{h}_u\right\|_2 \cdot\left\|\mathbf{e}_u^m\right\|_2\right)
\end{aligned}
$$
其中，$s(\cdot)$ 表示相似性函数，$\tau^{\prime}$ 是温度系数。我们的跨模态对比学习旨在学习一个嵌入空间，在这个空间中，不同用户表示之间相距很远，这使得我们的模型能够捕获到具有均匀分布嵌入的各种用户模态特定偏好。

3.3 多任务模型训练

为了生成我们最终的用户（项目）表示 $\mathbf{h}_u, \mathbf{h}_i \in \mathbb{R}^d$ 以进行预测，我们通过聚合它们对应的编码嵌入来促进协同视图和多模态视图之间的合作，如下所示：
$$
\mathbf{h}_u=\hat{\mathbf{e}}_u+\omega \sum_{m \in \mathcal{M}}^{|\mathcal{M}|} \frac{\mathbf{f}_u^m}{\left\|\mathbf{f}_u^m\right\|_2} ; \quad \mathbf{h}_i=\hat{\mathbf{e}}_i+\omega \sum_{m \in \mathcal{M}}^{|\mathcal{M}|} \frac{\mathbf{f}_i^m}{\left\|\mathbf{f}_i^m\right\|_2}
$$
其中，$\omega$ 是聚合权重。对编码视图（$\left(\hat{\mathbf{e}}_u, \hat{\mathbf{e}}_i\right)$）和多模态视图（$\left(\mathbf{f}_u^m, \mathbf{f}_i^m\right)$）的嵌入进行归一化，以减轻值尺度差异。通过最终的嵌入，我们的 MMSSL 模型通过 $\hat{y}_{u, i}=\mathbf{h}_u^{\top} \cdot \mathbf{h}_i$ 对用户 $u$ 和项目 $i$ 之间的未观察到的交互进行预测。

模型优化。我们使用多任务学习方案训练我们的推荐系统，同时优化 MMSSL 和以下任务：

i) 主要的用户-项目交互预测任务 $\mathcal{L}_{\mathrm{BPR}}$；

ii) 对抗性的模态感知用户-项目关系学习任务 $\mathcal{L}_{\mathrm{G}}$；

iii) 跨模态对比学习任务 $\mathcal{L}_{\mathrm{CL}}$。形式化地，联合优化的目标为（$\lambda_2, \lambda_3, \lambda_4$ 是损失项加权的超参数）：
$$
\begin{aligned}
& \mathcal{L}=\mathcal{L}_{\mathrm{BPR}}+\lambda_2 \cdot \mathcal{L}_{\mathrm{CL}}+\lambda_3 \cdot \mathcal{L}_{\mathrm{G}}+\lambda_4 \cdot\|\Theta\|^2 \\
& \mathcal{L}_{\mathrm{BPR}}=\sum_{\left(u, i_p, i_n\right)}^{|\mathcal{E}|}-\log \left(\operatorname{sigm}\left(\hat{y}_{u, i_p}-\hat{y}_{u, i_n}\right)\right)
\end{aligned}
$$
其中，$i_p, i_n$ 表示用户 $u$ 的正样本和负样本。在 $\mathcal{L}$ 中的最后一项是针对过拟合的权重衰减正则化。















对抗学习：

这段描述了一个对抗式自我监督学习（SSL）框架在多模态推荐系统中的应用，特别关注于如何通过调整学习到的模态感知用户-项目关系和观察到的用户-项目交互之间的分布差异来提高推荐系统的性能。具体地说，它涉及以下关键概念和步骤：

1. **对抗式SSL损失定义**：目标是最小化生成器 $\mathcal{G}$ 和最大化鉴别器 $\mathcal{D}$ 的性能，使得生成器能够生成接近真实用户-物品交互数据分布的模态感知用户-项目关系，而鉴别器则尝试区分生成的数据和真实数据。这种最小最大化策略的表达式是：
   $
   \min _{\mathcal{G}} \max _{\mathcal{D}} \mathbb{E}_{\tilde{\mathbf{A}} \sim \mathbb{P}_r}[\mathcal{D}(\tilde{\mathbf{A}})]-\mathbb{E}_{\hat{\mathbf{A}}^m \sim \mathbb{P}_f}\left[\mathcal{D}\left(\mathcal{G}\left(\mathbf{f}^m\right)\right)\right]
   ]$
   这里，$\mathbb{E}_{\tilde{\mathbf{A}} \sim \mathbb{P}_r}[\mathcal{D}(\tilde{\mathbf{A}})]$ 是鉴别器 $\mathcal{D}$ 对真实数据分布 $\mathbb{P}_r$ 的期望输出，而 $\mathbb{E}_{\hat{\mathbf{A}}^m \sim \mathbb{P}_f}[\mathcal{D}(\mathcal{G}(\mathbf{f}^m))]$ 是鉴别器对生成器 $\mathcal{G}$ 生成的数据分布 $\mathbb{P}_f$ 的期望输出。

2. **生成器和鉴别器的损失函数**：生成器的损失函数 $\mathcal{L}_{\mathcal{G}}$ 旨在最小化鉴别器对生成数据的识别准确性，即鼓励生成器生成难以被鉴别器区分的数据。鉴别器的损失函数 $\mathcal{L}_{\mathcal{D}}$ 包括两部分：一部分是识别真实数据的能力，另一部分是识别生成数据的能力，再加上一个梯度惩罚项，用于提高训练的稳定性和防止模式崩溃。

3. **WassersteinGANGP和梯度惩罚**：通过引入Wasserstein GAN with Gradient Penalty (WassersteinGANGP) 来添加梯度惩罚，这是为了确保训练过程的稳定性，减轻生成器和鉴别器之间的分布差异，使生成的数据更接近真实数据分布。梯度惩罚项 $\lambda_1 \mathbb{E}_{\ddot{\mathbf{A}}}\left[\left(\left\|\nabla_{\mathcal{D}(\ddot{\mathbf{A}})}\right\|-1\right)^2\right]$ 用于约束鉴别器在其输入上的梯度，以避免过于激进的优化策略，其中 $\ddot{\mathbf{A}}$ 是 $\hat{\mathbf{A}}^m$ 和 $\tilde{\mathbf{A}}$ 的插值表示，用于在优化过程中平滑处理两种分布。







* ## 对抗网络在这里是？

这两个公式分别描述了在对抗式自监督学习（SSL）框架中生成器（$\mathcal{G}$）和鉴别器（$\mathcal{D}$）的损失函数。这个框架是用来优化多模态推荐系统中的用户-物品交互预测。下面逐个详细解释这两个公式：

### 生成器的损失函数 $\mathcal{L}_{\mathcal{G}}$

生成器的目标是生成接近于真实用户-物品交互的模态感知用户-项目关系 $\hat{\mathbf{A}}^m$，以欺骗鉴别器，使其难以区分生成的交互数据和真实的交互数据。

- $\mathcal{L}_{\mathcal{G}}=-\mathbb{E}_{\hat{\mathbf{A}}^m}\left[\mathcal{D}\left(\hat{\mathbf{A}}^m\right)\right]$

这个公式中，$\mathbb{E}_{\hat{\mathbf{A}}^m}\left[\mathcal{D}\left(\hat{\mathbf{A}}^m\right)\right]$ 表示鉴别器 $\mathcal{D}$ 对于生成器 $\mathcal{G}$ 生成的数据 $\hat{\mathbf{A}}^m$ 的预测结果的期望值。生成器的损失是这个期望值的负值，意味着生成器 $\mathcal{G}$ 的训练过程是试图最大化鉴别器 $\mathcal{D}$ 对生成数据 $\hat{\mathbf{A}}^m$ 判定为真实数据的概率。

### 鉴别器的损失函数 $\mathcal{L}_{\mathcal{D}}$

鉴别器的目标是准确区分生成的用户-物品交互数据 $\hat{\mathbf{A}}^m$ 和真实的用户-物品交互数据 $\tilde{\mathbf{A}}$。

- $\mathcal{L}_{\mathcal{D}}=\mathbb{E}_{\tilde{\mathbf{A}}}[\mathcal{D}(\tilde{\mathbf{A}})]-\mathbb{E}_{\hat{\mathbf{A}}^m}\left[\mathcal{D}\left(\hat{\mathbf{A}}^m\right)\right]+\lambda_1 \mathbb{E}_{\ddot{\mathbf{A}}}\left[\left(\left\|\nabla_{\mathcal{D}(\ddot{\mathbf{A}})}\right\|-1\right)^2\right]$

这个公式由三部分组成：

1. $\mathbb{E}_{\tilde{\mathbf{A}}}[\mathcal{D}(\tilde{\mathbf{A}})]$ 是鉴别器 $\mathcal{D}$ 对真实数据 $\tilde{\mathbf{A}}$ 的预测结果的期望值，鉴别器希望这个值尽可能大。
2. $-\mathbb{E}_{\hat{\mathbf{A}}^m}\left[\mathcal{D}\left(\hat{\mathbf{A}}^m\right)\right]$ 是鉴别器 $\mathcal{D}$ 对生成数据 $\hat{\mathbf{A}}^m$ 的预测结果的期望值的负值，鉴别器希望这个值尽可能小，即希望能够识别出生成的数据。
3. $\lambda_1 \mathbb{E}_{\ddot{\mathbf{A}}}\left[\left(\left\|\nabla_{\mathcal{D}(\ddot{\mathbf{A}})}\right\|-1\right)^2\right]$ 是梯度惩罚项，用于确保鉴别器在其决策边界附近是平滑的，防止鉴别器过于自信地做出判断。$\ddot{\mathbf{A}}$ 表示 $\hat{\mathbf{A}}^m$ 和 $\tilde{\mathbf{A}}$ 的插值，$\lambda_1$ 是平衡该惩罚项影响的权重系数。

整体来看，这个框架通过对抗训练方法优化生成器和鉴别器，以生成更准确的用户-物品交互预测，同时引入梯度惩罚来提高训练的稳定性和防止模式崩溃。

* ## Q：这个的目的是为了什么，到底是谁更真谁更假

这个对抗式自我监督学习（SSL）框架的目的是提高多模态推荐系统中用户-物品交互预测的准确性和鲁棒性。通过生成器（$\mathcal{G}$）和鉴别器（$\mathcal{D}$）的对抗训练，框架能够学习到能更好地捕捉用户偏好和物品特征的多模态交互模式。在这个过程中，"真"与"假"的概念是相对的，用来指代数据的来源——是直接从用户-物品交互观测中得到的（真实数据），还是由模型生成的（生成数据）。

### 目的概述

- **提升预测性能**：通过对抗式训练，生成器学习生成越来越接近真实用户-物品交互数据的模态感知交互，以此来提高推荐系统的预测性能。
- **增强模型的泛化能力**：对抗式训练过程中，生成器和鉴别器不断地相互优化，有助于模型学习到更为复杂和深层次的数据表示，从而增强模型在未见过的数据上的泛化能力。
- **解决数据稀疏问题**：在推荐系统中，经常会遇到数据稀疏的问题，对抗式SSL可以通过生成缺失的用户-物品交互数据来缓解这一问题，从而提供更全面的数据视图以供学习。

### 谁更真谁更假

- **生成器的目标**是生成尽可能“真实”的用户-物品交互数据，即欺骗鉴别器使其不能区分生成数据和真实数据。
- **鉴别器的目标**是区分出哪些是真实的用户-物品交互数据，哪些是由生成器生成的数据。

在这个对抗过程中，不存在绝对的“真”或“假”。相反，通过这种对抗性的动态过程，模型能够逐步提高其在多模态推荐任务中处理和理解复杂用户-物品交互的能力。生成器试图生成越来越难以被鉴别器区分的数据，而鉴别器则试图提高其识别真实与生成数据的准确性。最终目的是通过这种动态的对抗过程提升整个推荐系统的性能，而不是单纯地判定哪些数据更真或更假。

* ## Q：生成器和判别器和预测模型分别是什么，生成器预测模型是一个东西？

在对抗式自我监督学习（SSL）框架，特别是在多模态推荐系统中，生成器（Generator）、鉴别器（Discriminator）和预测模型（Prediction Model）扮演着不同的角色。虽然它们共同工作以提高系统的性能，但它们各自有着不同的目的和工作机制。理解它们之间的区别有助于更好地把握整个系统的工作原理。

### 生成器（Generator）

- **作用**：生成器的主要任务是生成尽可能接近真实数据的新数据。在多模态推荐系统中，生成器尝试生成的是用户-物品交互数据，这些数据在特征上应与真实用户-物品交互数据尽可能相似。
- **与预测模型的关系**：生成器不直接进行最终的用户-物品推荐预测。它更多地是在数据层面上工作，**通过生成数据帮助模型学习到更丰富的数据表示，而不是直接输出推荐结果。**

### 鉴别器（Discriminator）

- **作用**：鉴别器的任务是区分输入数据是真实的用户-物品交互数据还是由生成器生成的。通过这种方式，鉴别器帮助指导生成器产生更加逼真的数据。
- **与预测模型的关系**：鉴别器同样不直接参与到最终的推荐预测中。它的作用主要是在训练过程中评估生成器的输出，从而促使生成器改进。

### 预测模型（Prediction Model）

- **作用**：预测模型是直接用于推荐系统的用户-物品交互预测的部分。它基于用户和物品的特征（包括由生成器产生的数据）来预测用户对物品的偏好或评分，从而完成推荐任务。
- **与生成器的关系**：预测模型可以使用生成器生成的数据作为训练数据来提高其预测的准确性和鲁棒性。在某些框架中，预测模型可能直接集成了生成器的输出作为其输入的一部分，但它们在功能上是分开的。

### 结论

生成器和预测模型不是同一个东西。生成器在对抗式学习环境中生成数据，试图模仿真实的用户-物品交互数据。而预测模型则使用包括真实数据和生成数据在内的所有可用数据来训练，以便更准确地预测用户对物品的偏好。预测模型是推荐系统的核心部分，直接决定了推荐结果的质量。在整个对抗式SSL框架中，生成器和鉴别器通过提供额外的、模仿真实情境的数据和反馈，辅助预测模型更好地学习和泛化。

* ## Q：所以对抗网络是通过生成数据来数据增强来实现自监督？

对抗网络（Generative Adversarial Networks, GANs）通过生成数据的方式可以被用于数据增强，但其在自我监督学习（Self-Supervised Learning, SSL）上的应用是更为广泛和深入的。对抗网络不仅仅通过数据增强来实现自监督学习，而是通过内部的对抗过程促进模型学习到更深层次、更丰富的数据表示。下面详细解释这两个方面：

### 数据增强

- **数据生成**：对抗网络通过生成器产生新的数据实例，这些数据在视觉上或统计特性上与真实数据类似，但并不存在于原始数据集中。这种生成的数据可以被用作数据增强，特别是在数据稀缺的情况下，帮助模型学习到更广泛的数据分布，从而提高模型的泛化能力。
- **数据多样性**：在多模态推荐系统中，对抗网络能够生成与原有物品相似但又略有差异的新物品特征，增加了数据的多样性，这对于缓解过拟合和提高推荐系统的鲁棒性非常有帮助。

### 自我监督学习

- **内部对抗过程**：GANs 的核心在于生成器和鉴别器之间的对抗过程。这一过程本质上是一个自监督过程，因为鉴别器试图区分真实数据与生成数据，而生成器则尝试生成尽可能逼真的数据来“欺骗”鉴别器。这种内部的动态对抗过程使得模型能够自我学习和自我优化，无需外部标注数据。
- **学习数据表示**：通过对抗训练，模型能够学习到能够捕捉到数据内在结构和模式的表示。对于推荐系统而言，这意味着能够更好地理解用户偏好和物品特性，进而提高推荐的精确度和满意度。

### 结论

因此，虽然对抗网络可以用于数据增强，但它们在自我监督学习中的作用远不止于此。通过内部的对抗过程，GANs 实现了一种强大的自我学习机制，使得模型能够在没有外部监督标签的情况下，自主学习到复杂的数据表示，进而提高各种任务，如推荐系统、图像识别、语言模型等的性能。

















代码：

```python
# 设置训练批次数量
n_batch = data_generator.n_train // args.batch_size + 1
best_recall = 0
for epoch in range(args.epoch):  # 训练循环开始
    t1 = time()  # 开始时间
    # 初始化各种损失值
    loss, mf_loss, emb_loss, reg_loss = 0., 0., 0., 0.
    contrastive_loss = 0.
    n_batch = data_generator.n_train // args.batch_size + 1
    sample_time = 0.
    # 初始化一些变量和字典
    self.gene_u, self.gene_real, self.gene_fake = None, None, {}
    self.topk_p_dict, self.topk_id_dict = {}, {}

    for idx in tqdm(range(n_batch)):  # 对每个批次进行迭代
        self.model.train()  # 设置模型为训练模式
        sample_t1 = time()
        # 从数据生成器中采样用户、正样本和负样本
        users, pos_items, neg_items = data_generator.sample()
        sample_time += time() - sample_t1       

        # 使用torch.no_grad()来停止计算梯度，节省内存和计算资源
        with torch.no_grad():
            # 获取不同类型的嵌入表示
            ua_embeddings, ia_embeddings, image_item_embeds, text_item_embeds, image_user_embeds, text_user_embeds, _, _, _, _, _, _ = self.model(self.ui_graph, self.iu_graph, self.image_ui_graph, self.image_iu_graph, self.text_ui_graph, self.text_iu_graph)
        
        # 计算用户和物品的相似度，并停止梯度计算
        ui_u_sim_detach = self.u_sim_calculation(users, ua_embeddings, ia_embeddings).detach()
        image_u_sim_detach = self.u_sim_calculation(users, image_user_embeds, image_item_embeds).detach()
        text_u_sim_detach = self.u_sim_calculation(users, text_user_embeds, text_item_embeds).detach()
        
        # 准备对抗训练的输入
        inputf = torch.cat((image_u_sim_detach, text_u_sim_detach), dim=0)
        predf = (self.D(inputf))
        lossf = (predf.mean())
        
        # 使用softmax和归一化处理原始UI图
        u_ui = torch.tensor(self.ui_graph_raw[users].todense()).cuda()
        u_ui = F.softmax(u_ui - args.log_log_scale*torch.log(-torch.log(torch.empty((u_ui.shape[0], u_ui.shape[1]), dtype=torch.float32).uniform_(0,1).cuda()+1e-8)+1e-8)/args.real_data_tau, dim=1)  
        u_ui += ui_u_sim_detach*args.ui_pre_scale                  
        u_ui = F.normalize(u_ui, dim=1)  
        
        # 准备对抗训练的真实输入
        inputr = torch.cat((u_ui, u_ui), dim=0)
        predr = (self.D(inputr))
        lossr = - (predr.mean())
        
        # 创新点引入
        gp = self.gradient_penalty(self.D, inputr, inputf.detach())
        
        # 计算判别器的总损失
        loss_D = lossr + lossf + args.gp_rate*gp 
        self.optim_D.zero_grad()
        loss_D.backward()
        self.optim_D.step()
        line_d_loss.append(loss_D.detach().data)

        # 获取生成模型的嵌入表示
        G_ua_embeddings, G_ia_embeddings, G_image_item_embeds, G_text_item_embeds, G_image_user_embeds, G_text_user_embeds, G_user_emb, _, G_image_user_id, G_text_user_id, _, _ = self.model(self.ui_graph, self.iu_graph, self.image_ui_graph, self.image_iu_graph, self.text_ui_graph, self.text_iu_graph)

        # 计算BPR损失
        G_u_g_embeddings = G_ua_embeddings[users]
        G_pos_i_g_embeddings = G_ia_embeddings[pos_items]
        G_neg_i_g_embeddings = G_ia_embeddings[neg_items]
        G_batch_mf_loss, G_batch_emb_loss, G_batch_reg_loss = self.bpr_loss(G_u_g_embeddings, G_pos_i_g_embeddings, G_neg_i_g_embeddings)
        
        G_image_u_sim = self.u_sim_calculation(users, G_image_user_embeds, G_image_item_embeds)
        G_text_u_sim = self.u_sim_calculation(users, G_text_user_embeds, G_text_item_embeds)
        G_image_u_sim_detach = G_image_u_sim.detach() 
        G_text_u_sim_detach = G_text_u_sim.detach()

        # 在特定间隔更新图结构、
        if idx % args.T == 0 and idx != 0:
            # 更新临时图结构，准备下一次的图更新
            self.image_ui_graph_tmp = csr_matrix((torch.ones(len(self.image_ui_index['x'])), (self.image_ui_index['x'], self.image_ui_index['y'])), shape=(self.n_users, self.n_items))
            self.text_ui_graph_tmp = csr_matrix((torch.ones(len(self.text_ui_index['x'])), (self.text_ui_index['x'], self.text_ui_index['y'])), shape=(self.n_users, self.n_items))
            self.image_iu_graph_tmp = self.image_ui_graph_tmp.T
            self.text_iu_graph_tmp = self.text_ui_graph_tmp.T
            self.image_ui_graph = self.sparse_mx_to_torch_sparse_tensor(
                self.csr_norm(self.image_ui_graph_tmp, mean_flag=True)
            ).cuda()
            self.text_ui_graph = self.sparse_mx_to_torch_sparse_tensor(
                self.csr_norm(self.text_ui_graph_tmp, mean_flag=True)
            ).cuda()
            self.image_iu_graph = self.sparse_mx_to_torch_sparse_tensor(
                self.csr_norm(self.image_iu_graph_tmp, mean_flag=True)
            ).cuda()
            self.text_iu_graph = self.sparse_mx_to_torch_sparse_tensor(
                self.csr_norm(self.text_iu_graph_tmp, mean_flag=True)
            ).cuda()

            self.image_ui_index = {'x': [], 'y': []}
            self.text_ui_index = {'x': [], 'y': []}

        else:
          	# 基于生成模型的相似度更新图的索引
            _, image_ui_id = torch.topk(G_image_u_sim_detach, int(self.n_items * args.m_topk_rate), dim=-1)
            self.image_ui_index['x'] += np.array(torch.tensor(users).repeat(1, int(self.n_items * args.m_topk_rate)).view(-1)).tolist()
            self.image_ui_index['y'] += np.array(image_ui_id.cpu().view(-1)).tolist()
            _, text_ui_id = torch.topk(G_text_u_sim_detach, int(self.n_items * args.m_topk_rate), dim=-1)
            self.text_ui_index['x'] += np.array(torch.tensor(users).repeat(1, int(self.n_items * args.m_topk_rate)).view(-1)).tolist()
            self.text_ui_index['y'] += np.array(text_ui_id.cpu().view(-1)).tolist()

        # 计算特征嵌入的正则化损失
        feat_emb_loss = self.feat_reg_loss_calculation(G_image_item_embeds, G_text_item_embeds, G_image_user_embeds, G_text_user_embeds)

        # 计算批量对比损失
        batch_contrastive_loss = 0
        batch_contrastive_loss1 = self.batched_contrastive_loss(G_image_user_id[users],G_user_emb[users])
        batch_contrastive_loss2 = self.batched_contrastive_loss(G_text_user_id[users],G_user_emb[users])
        batch_contrastive_loss = batch_contrastive_loss1 + batch_contrastive_loss2 

        # 使用生成器预测对抗训练的输出并计算损失
        G_inputf = torch.cat((G_image_u_sim, G_text_u_sim), dim=0)
        G_predf = (self.D(G_inputf))
        G_lossf = -(G_predf.mean())

        # 计算总损失并进行反向传播
        batch_loss = G_batch_mf_loss + G_batch_emb_loss + G_batch_reg_loss + feat_emb_loss + args.cl_rate*batch_contrastive_loss + args.G_rate*G_lossf  #feat_emb_loss
        self.optimizer_D.zero_grad()  
        batch_loss.backward(retain_graph=False)
        self.optimizer_D.step()

        # 累加损失
        loss += float(batch_loss)
        mf_loss += float(G_batch_mf_loss)
        emb_loss += float(G_batch_emb_loss)
        reg_loss += float(G_batch_reg_loss)

    # 清理不再需要的变量以节省内存
    del ua_embeddings, ia_embeddings, G_ua_embeddings, G_ia_embeddings, G_u_g_embeddings, G_neg_i_g_embeddings, G_pos_i_g_embeddings

    # 检查损失是否为nan，如果是则退出
    if math.isnan(loss) == True:
        self.logger.logging('ERROR: loss is nan.')
        sys.exit()

    # 记录训练过程的性能
    if (epoch + 1) % args.verbose != 0:
        perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f + %.5f  + %.5f]' % (
            epoch, time() - t1, loss, mf_loss, emb_loss, reg_loss, contrastive_loss)
        training_time_list.append(time() - t1)
        self.logger.logging(perf_str)

    t2 = time()
    # 测试模型性能
    users_to_test = list(data_generator.test_set.keys())
    users_to_val = list(data_generator.val_set.keys())
    
    ret = self.test(users_to_val, is_val=True)  
    training_time_list.append(t2 - t1)

    t3 = time()

    # 记录测试结果
    loss_loger.append(loss)
    rec_loger.append(ret['recall'].data)
    pre_loger.append(ret['precision'].data)
    ndcg_loger.append(ret['ndcg'].data)
    hit_loger.append(ret['hit_ratio'].data)

    line_var_recall.append(ret['recall'][1])
    line_var_precision.append(ret['precision'][1])
    line_var_ndcg.append(ret['ndcg'][1])

```



相似性计算代码：

```python
def u_sim_calculation(self, users, user_final, item_final):
    # 从用户的最终嵌入中选择当前批次用户的嵌入
    topk_u = user_final[users]
    
    # 将用户-物品图中当前用户的部分转换为稠密张量，并移至GPU
    u_ui = torch.tensor(self.ui_graph_raw[users].todense()).cuda()

    # 根据物品总数和批次大小计算需要的批次数量
    num_batches = (self.n_items - 1) // args.batch_size + 1
    
    # 生成包含所有物品索引的张量，并将其移至GPU
    indices = torch.arange(0, self.n_items).cuda()
    
    # 初始化列表，用于存储每个批次的用户-物品相似度
    u_sim_list = []

    # 遍历所有批次
    for i_b in range(num_batches):
        # 获取当前批次的物品索引
        index = indices[i_b * args.batch_size:(i_b + 1) * args.batch_size]
        
        # 计算当前批次中用户与物品的点积相似度
        sim = torch.mm(topk_u, item_final[index].T)
        
        # 调整相似度，忽略用户已知的正向交互，通过乘以（1-用户-物品关系图）
        sim_gt = torch.multiply(sim, (1-u_ui[:, index]))
        
        # 将调整后的相似度存入列表
        u_sim_list.append(sim_gt)
                
    # 将所有批次的相似度结果拼接，然后按行进行L2归一化
    u_sim = F.normalize(torch.cat(u_sim_list, dim=-1), p=2, dim=1)   
    
    # 返回归一化后的用户-物品相似度矩阵
    return u_sim

```



BPR_Loss:

```python
def bpr_loss(self, users, pos_items, neg_items):
    pos_scores = torch.sum(torch.mul(users, pos_items), dim=1)
    neg_scores = torch.sum(torch.mul(users, neg_items), dim=1)

    regularizer = 1. / 2 * (users ** 2).sum() + 1. / 2 * (pos_items ** 2).sum() + 1. / 2 * (neg_items ** 2).sum()
    regularizer = regularizer / self.batch_size

    maxi = F.logsigmoid(pos_scores - neg_scores)
    mf_loss = -torch.mean(maxi)

    emb_loss = self.decay * regularizer
    reg_loss = 0.0
    return mf_loss, emb_loss, reg_loss
```



多模态特征正则化

```python
def feat_reg_loss_calculation(self, g_item_image, g_item_text, g_user_image, g_user_text):
    feat_reg = 1. / 2 * (g_item_image ** 2).sum() + 1. / 2 * (g_item_text ** 2).sum() 
    						+ 1. / 2 * (g_user_image ** 2).sum() + 1. / 2 * (g_user_text ** 2).sum()
    feat_reg = feat_reg / self.n_items
    feat_emb_loss = args.feat_reg_decay * feat_reg
    return feat_emb_loss
```





```python

def positive_KCA(self, kge_model, positive_sample, h_or_t):
    relation_emb = kge_model.relation_embedding[positive_sample[:, 1]]  # b x dim
    pos_tail_index = positive_sample[:, h_or_t]
    pos_tail_emb = kge_model.entity_embedding[pos_tail_index]  # b x dim

    text_emb = self.relu(self.linear_text(self.ent_text_emb[pos_tail_index]))  # b x 4 x 200
    img_emb = self.relu(self.linear_img(self.ent_img_emb[pos_tail_index]))  # b x 24 x 200
    cross_mat = torch.matmul(img_emb, text_emb.permute(0, 2, 1))  # b x 24 x 4

    batchsize = relation_emb.size(0)

    #### BGA
    img_att = torch.matmul(torch.softmax(cross_mat.permute(0, 2, 1), dim=2), img_emb)  # b x 4 x 200
    rel_guided_img = torch.sigmoid(self.linear_rel1(relation_emb)).view(batchsize, 24, 4)  # B x 24 x 4
    rel_guided_img = torch.mul(rel_guided_img, cross_mat)  # B x 24 x 4
    img_att_rel_guided = torch.matmul(rel_guided_img.permute(0, 2, 1), img_emb)  # B x 4 x 200
    img_att_all = self.layernorm(img_att_rel_guided) + (img_att)  # B x 4 x 200

    text_att = torch.matmul(torch.softmax(cross_mat, dim=2), text_emb)  # b x 24 x 200
    rel_guided_text = torch.sigmoid(self.linear_rel2(relation_emb)).view(batchsize, 24, 4)  # B x 24 x 4
    rel_guided_text = torch.mul(rel_guided_text, cross_mat)  # B x 24 x 4
    text_att_rel_guided = torch.matmul(rel_guided_text, text_emb)  # B x 24 x 200
    text_att_all = self.layernorm(text_att_rel_guided) + self.layernorm(text_att)  # # B x 24 x 200

    img_att_all = self.linear_img_project(img_att_all.view(batchsize, -1))  # B  x dim
    text_att_all = self.linear_text_project(text_att_all.view(batchsize, -1))  # B  x dim

    att = torch.cat([pos_tail_emb, relation_emb], dim=1)
    att = self.relu(self.linear1(att))
    att = torch.sigmoid(self.linear3(att))  # B x dim
    pos_tail_emb = pos_tail_emb * att

    return img_att_all, text_att_all, pos_tail_emb

```

