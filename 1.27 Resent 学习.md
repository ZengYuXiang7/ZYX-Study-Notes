# 1.27 Resent 学习

层数越多效果越差

因为梯度消失提督爆炸造成问题

每一层误差越来越大

两种残差结构

* Q：什么是残差

  * 上一层输入作为下x层输入相加
  * 学习到输入和输出的差异
  * 直接学习到输入和输出

  残差连接（Residual Connection）是一种在深度学习模型中常见的结构，最初在2015年由何凯明等人在他们的论文《Deep Residual Learning for Image Recognition》中提出，主要用于解决深度神经网络中的梯度消失和梯度爆炸问题。

  在深度神经网络中，随着网络层的加深，输入数据经过多层非线性变换后，其特征可能会逐渐退化，导致网络难以训练。这种情况下，网络的输出与期望的输出之间可能存在一定差异，这个差异就是“残差”。

  具体来说，残差连接的核心思想是将某一层的输入直接连接到后面的某一层（通常是非相邻层），形成一个短路（shortcut）或跳跃连接（skip connection）。这样，深层的网络不是直接学习期望的输出，而是学习输入与输出之间的残差。如果输入和输出非常接近，这个残差就会很小，这样网络就只需要学习这个小的差异，从而简化了学习任务。

  残差连接的公式通常表示为：

  $[ H(x) = F(x) + x ]$

  其中，\( H(x) \) 是网络层的期望输出，\( x \) 是输入，\( F(x) \) 是网络需要学习的残差部分。这种结构有助于信息在网络中的流动，减少信息丢失，从而提高了网络的性能，尤其在构建深层网络时非常有效。

* Q：什么残差非常小，就有一个很好的效果呢

  * 但当输入和输出非常接近时，这意味着网络需要学习的映射接近于恒等映射（identity function），即直接输出输入值。

* Q：

  * 这意味着网络需要学习的映射接近于恒等映射（identity function），即直接输出输入值。残差网络通过直接将输入加到输出上，使得网络可以更容易地学习到这种接近恒等的映射。

* MASK：

  * Mask是可选的(opt.)，如果是能够获取到所有时刻的输入 (K, V), 那么就不使用Mask；如果是不能获取到，那么就需要使用Mask。使用了Mask 的Transformer型也被称为 Transformer Decoder，不使用 Mask的Transformer模型也被称为Transformer Encoder。