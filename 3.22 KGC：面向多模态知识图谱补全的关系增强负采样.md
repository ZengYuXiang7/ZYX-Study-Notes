# 3.22  KGC：面向多模态知识图谱补全的关系增强负采样
* **什么领域，什么问题**
  * 领域是多模态知识图谱补全（Multimodal Knowledge Graph Completion，MKGC），问题是如何利用多模态数据（如文本、图像）和知识图谱中的关系来改善知识图谱补全的性能，特别是通过改进负样本采样策略来解决模型训练中的消失梯度问题 。

* **作者做了什么**
  * 作者提出了一种新的多模态关系增强负样本采样框架（MMRNS），它包括知识引导的跨模态注意力机制和对比语义采样器，以及一种掩码Gumbel-Softmax优化机制，这些设计旨在充分利用多模态数据和复杂关系，提高负样本的质量 。

* **现有工作是怎么做的，有哪些欠考虑的**
  * 现有的MKGC方法主要通过均匀采样来构建负样本，这种方法在训练后期容易遇到消失梯度问题。尽管现有研究尝试通过生成对抗网络等方式来发现高质量的负样本，但这些技术主要集中在结构知识上，没有充分利用丰富的多模态线索，这限制了其在多模态知识图谱补全任务上的有效性 。

* **所以作者为什么选择了当前框架做法**
  * 作者认为，通过结合多模态数据和复杂的知识图谱关系，可以更有效地识别出难负样本，进而提高模型对知识图谱中缺失信息的补全能力。当前的框架通过知识引导的跨模态注意力机制来整合视觉和文本特征，通过对比学习进一步学习正负样本之间的语义相似度/差异，使用掩码Gumbel-Softmax机制解决采样过程的不可微问题，这种方法在提高负样本质量和优化参数方面都具有潜在优势 。

* **作者在实现框架过程, 遇到了什么挑战**
  * 在实现框架的过程中，一个主要挑战是如何模拟多模态数据和知识图谱关系的复杂交互，特别是如何通过关系来引导跨模态注意力的计算，以及如何设计有效的对比语义采样器和采样过程的可微优化策略 。

* **作者是怎么解决这些挑战的**
  * 为了解决这些挑战，作者提出了知识引导的跨模态注意力机制，该机制可以整合视觉和文本特征，通过关系嵌入来引导注意力的计算。此外，作者设计了对比语义采样器来进一步提升负样本的质量，并引入掩码Gumbel-Softmax机制来实现采样过程的可微优化，这些设计帮助框架有效地处理了多模态数据和复杂关系的交互 。

* 文章细节
  
  问题定义：
  
  ​	给定一个知识图谱 $\mathcal{G}=\{(h, r, t)\} \subseteq \mathcal{E} \times \mathcal{R} \times \mathcal{E}$，我们用 $\mathcal{E}$ 表示实体集合，用 $\mathcal{R}$ 表示关系集合。知识图谱中的每个三元组表示为 $(h, r, t)$，意味着头实体 $h \in \mathcal{E}$ 和尾实体 $t \in \mathcal{E}$ 通过有向关系 $r \in \mathcal{R}$ 连接。此外，我们用 $t \in \mathbb{R}^{d_{emb}}$ 和 $r \in \mathbb{R}^{d_{emb}}$ 分别表示尾实体和关系的嵌入。同时，我们用 $\boldsymbol{e}_{\boldsymbol{i}} \in \mathbb{R}^{d_i \times d_N}$ 表示视觉特征，用 $\boldsymbol{e}_{\boldsymbol{t}} \in \mathbb{R}^{d_t \times d_M}$ 表示文本特征来描述多模态线索。
  
  ​	这样，知识图谱补全（KGC）任务可以被建模为一个排序问题，即给定一个正例三元组 $\left(h, r, t^{+}\right)$ 和若干负例三元组 $\left(h, r, t^{-}\right)$，KGC 模型旨在通过一个有效的评分函数提高正例三元组的得分并降低负例三元组的得分。沿着这条线，我们采样策略的目标是利用正例三元组和相应的多模态数据来最大化难负例样本 $t^{-}$​​ 的采样概率，这些负例样本在语义上与正例相似，以提高模型的区分能力。
  
  ![image-20240322094715145](./assets/image-20240322094715145.png)
  
  知识交叉模态注意力
  
  ​	接下来，我们详细介绍知识引导的跨模态注意力（KCA）机制的细节，该机制通过整合多个关系来学习跨模态双向注意力权重。
  
  ​	具体来说，KCA首先尝试捕获不同模态之间的交互，即图像和文本，旨在同时突出跨模态数据之间相同的语义特征，以学习与关系无关的特征。我们通过跨模态特征来表示与关系无关的特征，这些特征在不同的关系下都很重要，用于识别难样本。例如，在图1中，泰勒·斯威夫特的负样本预期是一个与人相关的实体，包含更多与人体或面部相关的属性，而不管关系是AwardOf还是Girlfriend，都与其他不相关的实体（如位置）无关。
  
  ​	同时，在描述了多模态交互之后，KCA进一步整合关系信息，以指导模型确定哪些多模态语义特征应该被突出，以学习由关系引导的特征。例如，当关系是AwardOf时，KCA旨在增强如歌手和音乐等属性的跨模态注意力。当关系是Girlfriend时，KCA旨在增强如女性等属性的跨模态注意力。值得注意的是，关系作为一种分类数据，包含有限且粗粒度的标签信息，通常与图像和文本没有语义相似性或相关性。因此，在引入关系进行引导时，我们首先通过建模文本和视觉特征的交互，然后分别引入关系嵌入来指导图像和文本的跨模态注意力权重。
  
  ![image-20240322094644368](./assets/image-20240322094644368.png)
  
  ​	根据这一思路，给定视觉特征 $\boldsymbol{e}_{\boldsymbol{i}}$ 和文本特征 $\boldsymbol{e}_{\boldsymbol{t}}$，它们首先被输入到一个全连接网络中进行非线性映射和维度统一：
  $$
  \hat{\boldsymbol{e}}_{\boldsymbol{i}}=R\left(\boldsymbol{e}_{\boldsymbol{i}} \boldsymbol{W}_{\boldsymbol{i}}+\boldsymbol{b}_{\boldsymbol{i}}\right) \in \mathbb{R}^{d_i \times d_{att}} \quad \hat{\boldsymbol{e}}_{\boldsymbol{t}}=R\left(\boldsymbol{e}_{\boldsymbol{t}} \boldsymbol{W}_{\boldsymbol{t}}+\boldsymbol{b}_{\boldsymbol{t}}\right) \in \mathbb{R}^{d_t \times d_{att}}
  $$
  其中 $R(\cdot)$ 是激活函数 LeakyRELU，$\boldsymbol{W}$ 和 $\boldsymbol{b}$ 分别代表可训练的权重和偏置。跨模态矩阵 $\boldsymbol{M} \in \mathbb{R}^{d_i \times d_t}$ 通过下式计算：
  $$
  \boldsymbol{M}=\hat{\boldsymbol{e}}_{\boldsymbol{i}} \cdot \hat{\boldsymbol{e}}_{\boldsymbol{t}}^T
  $$
  $\boldsymbol{M}$ 的目标是捕获和突出图像和文本之间相同的语义特征。这里，该模块被划分为四个分支，如图3所示：(1) 文本引导的视觉注意力，(2) 关系-文本引导的视觉注意力，(3) 关系-图像引导的文本注意力，以及 (4) 图像引导的文本注意力。
  	在分支 (1) 中，KCA 标准化 $\boldsymbol{M}$ 来产生由文本的每个句子条件化的视觉区域的注意力权重。注意力权重乘以图像特征 $\hat{\boldsymbol{e}}_{\boldsymbol{i}}$ 来得到关注的与关系无关的视觉表示 $\boldsymbol{e}_{i r}^i$，这适用于任何关系类型。
  
  ​	在分支 (2) 中，KCA 旨在进一步整合关系嵌入来引导跨模态语义信息。与 (1) 的区别在于，(2) 利用知识图谱中的关系来引导标准化的注意力权重。在这种情况下，注意力权重也乘以图像特征 $\hat{\boldsymbol{e}}_{\boldsymbol{i}}$ 来得到关注的由关系引导的视觉表示 $\boldsymbol{e}_{g u}^i$：
  $$
  \boldsymbol{e}_{g u}^i=\hat{\boldsymbol{e}}_{\boldsymbol{i}} \cdot\left(\boldsymbol{M}^T \odot \boldsymbol{e}_{\boldsymbol{i}}^{\boldsymbol{r}}\right)
  $$
  这里的 $\odot$ 表示元素乘法操作，$\boldsymbol{e}_{\boldsymbol{i}}^{\boldsymbol{r}}$​ 代表关系的嵌入，用于调制注意力权重，从而使注意力机制可以根据关系的不同而突出不同的视觉特征。
  
  ​	这里 $\boldsymbol{e}_{\boldsymbol{i}}^r$ 和 $\boldsymbol{e}_{\boldsymbol{t}}^{\boldsymbol{r}}$ 是通过将关系 $r$ 的嵌入输入到两个不同的全连接层产生的，分别引导视觉和文本注意力的双向生成：
  $$
  \boldsymbol{e}_i^r=\sigma\left(\boldsymbol{r} \cdot W_i^r+b_i^r\right) \quad e_t^r=\sigma\left(\boldsymbol{r} \cdot W_t^r+b_t^r\right)
  $$
  
  相应地，分支 (3) 和 (4) 尝试学习由图像和关系引导的被关注的文本表示，其动机类似于分支 (2) 和 (1)。跨模态和关系引导的表示 $\boldsymbol{e}_{i r}^i$ 和 $\boldsymbol{e}_{g u}^i$ 都被输入到层归一化中以统一分布，然后相加得到知识引导的视觉表示：
  $$
  \boldsymbol{e}_{k c a}^i=\operatorname{Norm}\left(\boldsymbol{e}_{i r}^i\right)+\operatorname{Norm}\left(\boldsymbol{e}_{g u}^i\right)
  $$
  同样地，我们采用相同的过程产生知识引导的文本表示 $\boldsymbol{e}_{k c a}^t$​。
  
  对比语义采样之后，
  
  我们进一步构建了一个对比语义采样器，以计算负样本的采样分布。采样器首先应用预训练模型来提取语义特征，然后使用KCA机制来模拟多模态交互，并以关系为指导。我们采样器的核心是通过挖掘正样本和负样本之间的相似性和差异性，进一步学习多模态语义表示。
  
  3.3.1 特征预处理
  
  我们首先通过BEiT[3]提取初步视觉特征，这可以用于学习语义区域和对象边界。我们对语义视觉表示应用平均池化，以降低计算复杂性。然后，我们通过SBERT[30]提取初步文本特征，这在常见的语义文本相似性任务上有显著改进。此外，我们通过裁剪和填充使表示张量具有相同的维度。由于实体也是结构嵌入，如同关系一样，我们只需连接它们并将它们输入到全连接网络中以整合关系信息。
  $$
  \boldsymbol{e}_{\boldsymbol{s}}=\boldsymbol{t} \cdot \sigma\left(\operatorname{concat}(\boldsymbol{r}, \boldsymbol{t}) \cdot \boldsymbol{W}_{\boldsymbol{s}}+\boldsymbol{b}_{\boldsymbol{s}}\right)
  $$
  
  3.3.2 余弦相似性
  
  正样本和负样本的图像-文本对的初步特征分别被输入到KCA中。正样本和负样本的KCA共享参数。两个实体的视觉表示之间的视觉特征相似性 $z_i$ 和 $z_j$ 使用余弦相似性来测量。$\delta$ 是一个小数，以确保分母不为零。
  $$
  \operatorname{sim}^i\left(z_i, z_j\right)=\frac{z_i{ }^T z_j}{\left\|z_i\right\|\left\|z_j\right\|+\delta}
  $$
  3.3.3 对比损失
  
  最后，我们构建了一个与文献[10]类似的对比损失函数，该函数以相似性为输入，但仅具有一个正样本对。然而，由于知识图谱中的1对多关系，我们有多个正样本。
  
  因此，该模块的目标是缩小正样本之间的差距，同时扩大正样本和负样本之间的差距。此外，我们在框架中整合了自对抗技术[32]，以进一步提高模型性能。对于第 $i$ 个三元组的损失权重 $p\left(h_i, r, t_i\right)$ 由KGC模型的分数计算得出。对于那些未被采样的三元组，我们将其权重设置为 $1 /|\mathcal{E}|$：
  $$
  p\left(h_i, r, t_i\right)=\left\{\begin{array}{lr}
  \frac{\exp \left(\alpha \cdot K G C\left(h_i, r, t_i\right)\right)}{\sum_{j \in S} \exp \left(\alpha \cdot K G C\left(h_j, r, t_j\right)\right)} & i \in S \\
  1 /|\mathcal{E}| & \text { otherwise }
  \end{array}\right.
  $$
  其中，$S$ 是采样三元组的集合，$\alpha$ 是采样的温度。这样，视觉特征相似性的最终对比损失函数如下：
  $$
  l_{\text {con }}^i=-\log \frac{\sum_{j \in P} p\left(h_j, r, t_j\right) \exp \left(\operatorname{sim}^i\left(z, z_j\right)\right)}{\sum_{n \in N} p\left(h_n, r, t_n\right) \exp \left(\operatorname{sim}^i\left(z, z_n\right)\right)}
  $$
  其中，$P$ 是正样本集，$N$ 是负样本集。我们同时按照公式8计算文本和结构特征的相似性，以及按照公式10计算对比损失，分别表示为 $l_{\text {con }}^t$ 和 $l_{\text {con }}^s$。总的对比损失通过对它们取平均获得：
  $$
  L_{\text {con }}=\left(l_{\text {con }}^i+l_{\text {con }}^t+l_{\text {con }}^s\right) / 3
  $$
  在这一部分中，我们将解释如何使用提出的可微采样方法，该方法结合了掩码操作和gumbel-softmax来确保有效的梯度反向传播。掩码操作旨在解决将gumbel-softmax引入KGC采样过程中的问题。
  
  3.4.1 Gumbel-Softmax
  
  由于类别分布的采样过程与优化过程是独立的，KGC模型的梯度无法反向传播到采样网络。因此，对比语义采样器的可训练参数不能与KGC模型的训练阶段以端到端的方式优化。为了实现梯度反向传播，我们引入了gumbel-softmax的重参数化技巧[17]，它产生一个连续分布，可以使用softmax函数作为argmax的可微近似，从离散概率分布$\boldsymbol{p}$中近似采样：$\boldsymbol{y}=\operatorname{softmax}((\log (\boldsymbol{p})+\boldsymbol{g}) / \tau)$。其中，$\boldsymbol{g}$中的每个元素$g_i$是从标准Gumbel分布[14, 25]中独立同分布(i.i.d)采样得到的。
  
  3.4.2 掩码向量
  
  考虑到图像、文本和结构中正样本和负样本的语义相似性分别用于计算概率分布$\boldsymbol{p}$，我们使用softmax将相似性转换为采样概率：
  $$
  \boldsymbol{p}=\left(S F\left(\operatorname{sim}^i / \epsilon\right)+S F\left(\operatorname{sim}^t / \epsilon\right)+S F\left(\operatorname{sim}^s / \epsilon\right)\right) / 3
  $$
  其中$S F(\cdot)$表示softmax函数，$\epsilon$是一个用于平衡探索和利用的因子，将在下文中详细说明。然而，$\boldsymbol{p}$不是最终的采样概率分布。由于知识图谱中1对多关系非常常见，不是所有的实体都可以被视为负样本。最常见的方法将过滤掉正样本$[4,32]$。一种常见的做法是将采样概率分布$\boldsymbol{p}$中正样本的位置设为零。但这将使gumbel-softmax不可微，这与我们的目的相悖。因此，我们提出了一种非可微的掩码向量，其中负位置的值设置为1.0，正位置的值设置为接近零的数值。概率分布$\boldsymbol{p}$与掩码向量进行元素乘法。我们注意到，由于对数函数，乘法可以被加法替代，以减少计算复杂性。以下是掩码gumbel-softmax：
  $$
  \boldsymbol{y}_{\boldsymbol{m}}=S F((\log (\boldsymbol{p})+\log (\boldsymbol{m a s k})+\boldsymbol{g}) / \tau)
  $$
  这里$\boldsymbol{y}_{\boldsymbol{m}}$是采样结果。值得注意的是，掩码向量还有助于实现无替换采样。总损失$L$通过将KGC模型的损失$L_{k g c}$和采样器的损失$L_{\text {con }}$相加获得。损失率$\beta$的影响将在4.5节中分析。
  $$
  L=L_{k g c}+\beta L_{c o n}
  $$
  
  3.4.3 探索与利用
  
  ### 3.4.3 探索与利用
  在这里，考虑到在不同训练阶段使采样策略适应性强，我们进一步定义了一个探索与利用因子 $\epsilon$。其动机是在早期训练阶段学习难样本和简单样本，并在后期训练阶段更多地关注难样本的利用。$\epsilon$ 的值随着迭代次数的增加而减小。$\epsilon$ 的效果将在4.5节中详细讨论。
  $$
  \epsilon=\epsilon_o /(1+\log (\text { iter }))
  $$
