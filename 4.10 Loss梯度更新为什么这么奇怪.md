# 4.10 Loss梯度更新为什么这么奇怪



一个神经网络可以有两个损失函数吗?

我们知道在图像处理很多文章的损失函数由多个部分组成，但最后还是会加权合并成一个总的损失，能不能就是同时有两个损失函数，优化同一个网络，交替优化那种，类似于gan，就比如说召回率和精确率，两个有点互斥的感觉，我想两个指标都好，可以这样做吗?初学者，说的不对的地方希望大家指正



网友1:

Faster R-CNN在最初的论文发表（nips2015）之时，就是用不同的loss交替训练模型。具体地说，就是时而通过训练让Region Proposal更准确，时而固定所有的Region Proposals通过训练让最终的boxes更准确。在最初的论文发表之后，作者又实现了joint训练。并且作者声称joint训练的优化速度是交替训练的1.5倍。

作者：dawnbreaker
链接：https://www.zhihu.com/question/338559590/answer/781802397
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



网友2:

这里要区分是严格多目标还是弱多目标

你说的多个损失加权这种，就是弱多目标，多个目标都有考虑，但对于模型优化来说就是一个目标。你说的两个loss交替优化，本质还是融合了，你可以认为是0.5 0.5加权

再说严格多目标，这个不存在的！同一组参数，不可能同时往两个方向优化

实际应用中，本质都是弱多目标，融合寻找一个权衡。而弱多目标可以work的隐含假设是，多个目标存在不低的相关性，不可忽视

作者：Dylan
链接：https://www.zhihu.com/question/338559590/answer/780820885
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



多任务学习就是允许多个损失的，不过会加权组合在一起，一并更新梯度。其实这等价于每个损失去更新梯度。

召回率和精确率，都要好，似乎采用级联思想吧。





大家好，我是泰哥。同学们在日常训练模型的过程中，有没有对结果进行过如下观察比较：

- 使用`MSE损失函数`时，预测结果更加偏向异常值
- 而使用`MAE损失函数`训练时，`Loss`与`MSE`的`Loss`相差不多，但是预测结果却更加偏向常规值





![image-20240410135849831](./assets/image-20240410135849831.png)











作者：hzwer
链接：https://www.zhihu.com/question/375794498/answer/2292320194
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。





**这也是个困扰了我多年的问题：**

**loss = a * loss1 + b * loss2 + c * loss3 怎么设置 a，b，c？**

**我的经验是 loss 的尺度一般不太影响性能，除非本来主 loss 是 loss1，但是因为 b，c 设置太大了导致其他 loss 变成了主 loss。**

**实践上有几个调整方法：**

1. **手动把所有 loss 放缩到差不多的尺度，设 a = 1，b 和 c 取 10^k，k 选完不管了；**
2. **如果有两项 loss，可以 loss = a * loss1 + (1 - a) * loss2，通过控制一个超参数 a 调整 loss；**
3. **我试过的玄学躺平做法 loss = loss1 / loss1.detach() + loss2 / loss2.detach() + loss3 loss3.detach()，分母可能需要加 eps，相当于在每一个 iteration 选定超参数 a, b, c，使得多个 loss 尺度完全一致；进一步更科学一点就 loss = loss1 + loss2 / (loss2 / loss1).detach() + loss3 / (loss3 / loss1).detach()，感觉比 loss 向 1 对齐合理**

**某大佬告诉我，loss 就像菲涅尔透镜，纵使你能设计它的含义，也很难设计它的梯度。所以暴力一轮就躺平了。我个人见解，很多 paper 设计一堆 loss 只是为了让核心故事更完整，未必强过调参。**







### 关于正则化

![image-20240410144520446](./assets/image-20240410144520446.png)



# 所谓正则化参数到底是又什么意思？

在看一篇文献时，看到了文中提及用L曲线准则确定正则化参数。这里的正则化参数是什么意思？而且在后面进一步说明中提及，做出了一个函数曲线，并将该曲线的角点对应的值作为最合理的正则化参数，这又有什么理由吗？



目前在看机器学习，对于正则化我只能想到这方面的了，简单来说就是添加参数来防止曲线过拟合。

![img](./assets/v2-40a9f9b4aaf3f51f6174463ebda9366b_1440w.jpg)

对于拟合曲线，如果参数太少，那么会造成欠拟合，如果太多，就会过拟合，见蓝色的曲线，过拟合的原因是部分theta太大，这时候，对于所有拟合参数theta1,theta2 ... thetam，可以添加个平衡参数lambda，当lambda设置的很大的时候，那theta就必须缩小才能保证成本函数的最小值，这时候theta的权重就下降了，相应的就是曲线由蓝色转变为紫色，不再过拟合

作者：阿卡林
链接：https://www.zhihu.com/question/68222349/answer/264076169
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。





![image-20240410152307266](./assets/image-20240410152307266.png)

疏规则化关键原因在于它能实现特征的自动选择。一般来说，2的大部分元素（特征）都是和 最终的输出没有关系或者不提供任何信息的，在最小化目标函数的时候考虑这些額外的特 征，虽然可以获得更小的训练误差，但在预测新的

![image-20240410152323429](./assets/image-20240410152323429.png)

![image-20240410152342914](./assets/image-20240410152342914.png)

![image-20240410152412556](./assets/image-20240410152412556.png)





![image-20240410152430475](./assets/image-20240410152430475.png)

![image-20240410152439897](./assets/image-20240410152439897.png)

















# 这是一个写得非常好的博客（关于loss）：

https://c-harlin.github.io/学习笔记/2022/07/30/多任务学习概述.html

![image-20240410140949155](./assets/image-20240410140949155.png)

![image-20240410140959804](./assets/image-20240410140959804.png)

![image-20240410141023544](./assets/image-20240410141023544.png)

![image-20240410141036368](./assets/image-20240410141036368.png)

![image-20240410141050426](./assets/image-20240410141100005.png)

![image-20240410141118058](./assets/image-20240410141118058.png)

![image-20240410141126908](./assets/image-20240410141126908.png)

![image-20240410141135548](./assets/image-20240410141135548.png)







